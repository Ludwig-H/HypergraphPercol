{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I2yARqc9O0J8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2yARqc9O0J8",
        "outputId": "341e7d5b-26a6-4ae4-a8ff-a292f0c4b818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "# # 0) Réglages pour éviter la bagarre de threads sous-jacents\n",
        "# import os, faulthandler, sys\n",
        "# os.environ.update({\n",
        "#     \"PYTHONMALLOC\": \"debug\",              # hooks Python sur alloc\n",
        "#     \"MALLOC_CHECK_\": \"3\",                 # vérifs glibc malloc (héritage)\n",
        "#     \"GLIBC_TUNABLES\": \"glibc.malloc.check=3\",  # vérifs glibc récentes\n",
        "#     \"OMP_NUM_THREADS\": \"1\",               # pas de bagarre OpenMP/BLAS\n",
        "#     \"OPENBLAS_NUM_THREADS\": \"1\",\n",
        "#     \"MKL_NUM_THREADS\": \"1\",\n",
        "#     \"NUMEXPR_NUM_THREADS\": \"1\",\n",
        "#     \"VECLIB_MAXIMUM_THREADS\": \"1\",\n",
        "# })\n",
        "\n",
        "# faulthandler.enable()\n",
        "\n",
        "import os, sys\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "N_cpu = os.cpu_count() # 8\n",
        "print(N_cpu)\n",
        "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"       # au cas où miniball ou BLAS threadent\n",
        "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac535ff4",
      "metadata": {
        "id": "ac535ff4"
      },
      "outputs": [],
      "source": [
        "# # !pip install gudhi\n",
        "# # Colab / Jupyter\n",
        "# # %pip uninstall -y gudhi numpy scipy\n",
        "# # %pip install --no-cache-dir --force-reinstall \"numpy\" \"gudhi=3.10.*\" \"scipy>=1.14\" \"pybind11>=2.12\"\n",
        "# # Installe GUDHI 3.11.0 proprement dans Colab\n",
        "# %pip uninstall -y gudhi numpy -q\n",
        "# %pip install -q --no-cache-dir --force-reinstall --only-binary=:all: \\\n",
        "#   \"numpy==2.0.*\" \"gudhi==3.11.0\"\n",
        "\n",
        "# import sys, numpy as np, gudhi\n",
        "# print(\"Python:\", sys.version.split()[0],\n",
        "#       \"| NumPy:\", np.__version__,\n",
        "#       \"| GUDHI:\", gudhi.__version__)\n",
        "# import gudhi as gd\n",
        "# print(gd.__debug_info__)\n",
        "# print(gd.__available_modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75734f26",
      "metadata": {
        "id": "75734f26"
      },
      "outputs": [],
      "source": [
        "%load_ext cython\n",
        "%config Cython.annotate = True\n",
        "%config Cython.language_level = 3\n",
        "%reload_ext cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b845b30c",
      "metadata": {
        "id": "b845b30c"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "# cython: boundscheck=False\n",
        "# cython: nonecheck=False\n",
        "# cython: initializedcheck=False\n",
        "# Tree handling (condensing, finding stable clusters) for hdbscan\n",
        "# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION\n",
        "\n",
        "import numpy as np\n",
        "cimport numpy as np\n",
        "\n",
        "cdef np.double_t INFTY = np.inf\n",
        "\n",
        "cdef class UnionFind:\n",
        "    cdef np.intp_t[:] parent\n",
        "    cdef np.intp_t[:] _size\n",
        "    def __init__(self, int n):\n",
        "        self.parent = np.arange(n, dtype=np.intp)\n",
        "        self._size  = np.ones(n, dtype=np.intp)\n",
        "    cpdef int find(self, int x):\n",
        "        cdef int r=x\n",
        "        while self.parent[r]!=r:\n",
        "            r=self.parent[r]\n",
        "        cdef int cur = x\n",
        "        cdef int nxt\n",
        "        while self.parent[cur]!=r:\n",
        "            nxt=self.parent[cur]\n",
        "            self.parent[cur]=r\n",
        "            cur=nxt\n",
        "        return r\n",
        "    cpdef bint union(self,int x,int y):\n",
        "        cdef int rx=self.find(x)\n",
        "        cdef int ry=self.find(y)\n",
        "        if rx==ry: return False\n",
        "        if self._size[rx]<self._size[ry]:\n",
        "            self.parent[rx]=ry; self._size[ry]+=self._size[rx]\n",
        "        else:\n",
        "            self.parent[ry]=rx; self._size[rx]+=self._size[ry]\n",
        "        return True\n",
        "    cpdef int component_size(self,int x):\n",
        "        return self._size[self.find(x)]\n",
        "\n",
        "\n",
        "\n",
        "ctypedef np.double_t DTYPE_t\n",
        "ctypedef np.int64_t  ITYPE_t\n",
        "\n",
        "# Calcule (q, w) pour un k-uplet d'indices, avec s2_all = ||p_i||^2 pré-calculé.\n",
        "# w = ||q||^2 - mean(s2_all[idx])\n",
        "cpdef double bary_weight_one(DTYPE_t[:, ::1] M, DTYPE_t[::1] s2_all,\n",
        "                             ITYPE_t[::1] idx, DTYPE_t[::1] out_q):\n",
        "    cdef Py_ssize_t k = idx.shape[0]\n",
        "    cdef Py_ssize_t d = M.shape[1]\n",
        "    cdef Py_ssize_t i, t\n",
        "    cdef double smean = 0.0\n",
        "    cdef double qnorm2 = 0.0\n",
        "    cdef ITYPE_t ii\n",
        "\n",
        "    # accumuler les sommes des coordonnées\n",
        "    for t in range(d):\n",
        "        out_q[t] = 0.0\n",
        "\n",
        "    for i in range(k):\n",
        "        ii = idx[i]\n",
        "        smean += s2_all[ii]\n",
        "        for t in range(d):\n",
        "            out_q[t] += M[ii, t]\n",
        "\n",
        "    # moyenne et poids\n",
        "    for t in range(d):\n",
        "        out_q[t] /= k\n",
        "        qnorm2 += out_q[t] * out_q[t]\n",
        "\n",
        "    smean /= k\n",
        "    return qnorm2 - smean\n",
        "\n",
        "\n",
        "# Version batch: combos (m,k) -> Q (m,d) et w (m,)\n",
        "cpdef void bary_weight_batch(DTYPE_t[:, ::1] M, DTYPE_t[::1] s2_all,\n",
        "                             ITYPE_t[:, ::1] combos,\n",
        "                             DTYPE_t[:, ::1] out_Q, DTYPE_t[::1] out_w):\n",
        "    cdef Py_ssize_t m = combos.shape[0]\n",
        "    cdef Py_ssize_t k = combos.shape[1]\n",
        "    cdef Py_ssize_t d = M.shape[1]\n",
        "    cdef Py_ssize_t i, j, t\n",
        "    cdef double smean, qnorm2\n",
        "    cdef ITYPE_t ii\n",
        "\n",
        "    for i in range(m):\n",
        "        smean = 0.0\n",
        "        # reset ligne de Q\n",
        "        for t in range(d):\n",
        "            out_Q[i, t] = 0.0\n",
        "        # accumulateurs\n",
        "        for j in range(k):\n",
        "            ii = combos[i, j]\n",
        "            smean += s2_all[ii]\n",
        "            for t in range(d):\n",
        "                out_Q[i, t] += M[ii, t]\n",
        "        # moyenne des coordonnées\n",
        "        for t in range(d):\n",
        "            out_Q[i, t] /= k\n",
        "        # poids\n",
        "        smean /= k\n",
        "        qnorm2 = 0.0\n",
        "        for t in range(d):\n",
        "            qnorm2 += out_Q[i, t] * out_Q[i, t]\n",
        "        out_w[i] = qnorm2 - smean\n",
        "\n",
        "\n",
        "cpdef int union_if_adjacent_int(ITYPE_t[::1] a, ITYPE_t[::1] b, ITYPE_t[::1] out_u):\n",
        "    cdef Py_ssize_t k = a.shape[0]\n",
        "    cdef Py_ssize_t i = 0\n",
        "    cdef Py_ssize_t j = 0\n",
        "    cdef Py_ssize_t u = 0\n",
        "\n",
        "    while i < k and j < k:\n",
        "        if u >= out_u.shape[0]:\n",
        "            return 0\n",
        "        if a[i] == b[j]:\n",
        "            out_u[u] = a[i]; i += 1; j += 1; u += 1\n",
        "        elif a[i] < b[j]:\n",
        "            out_u[u] = a[i]; i += 1; u += 1\n",
        "        else:\n",
        "            out_u[u] = b[j]; j += 1; u += 1\n",
        "\n",
        "    while i < k:\n",
        "        if u >= out_u.shape[0]:\n",
        "            return 0\n",
        "        out_u[u] = a[i]; i += 1; u += 1\n",
        "\n",
        "    while j < k:\n",
        "        if u >= out_u.shape[0]:\n",
        "            return 0\n",
        "        out_u[u] = b[j]; j += 1; u += 1\n",
        "\n",
        "    return 1 if u == k + 1 else 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cdef inline np.int64_t _min_i64(np.int64_t a, np.int64_t b) nogil:\n",
        "    return a if a <= b else b\n",
        "\n",
        "cdef inline np.int64_t _max_i64(np.int64_t a, np.int64_t b) nogil:\n",
        "    return a if a >= b else b\n",
        "\n",
        "cpdef tuple build_leaf_dfs_intervals(np.ndarray[np.int64_t, ndim=1] left,\n",
        "                                      np.ndarray[np.int64_t, ndim=1] right):\n",
        "    \"\"\"\n",
        "    Retourne:\n",
        "      pos        (m,)     int64  leaf_id -> rang DFS\n",
        "      first,last (m+t,)   int64  intervalle de feuilles par nœud [first,last]\n",
        "      leaf_order (m,)     int64  rang DFS -> leaf_id\n",
        "    Hypothèse SciPy: parent i = m+i, enfants < parent.\n",
        "    \"\"\"\n",
        "    cdef Py_ssize_t t = left.shape[0]\n",
        "    if right.shape[0] != t:\n",
        "        raise ValueError(\"left/right must have same length\")\n",
        "    cdef Py_ssize_t m = t + 1\n",
        "    cdef Py_ssize_t n_nodes = m + t\n",
        "\n",
        "    cdef np.int64_t[:] L = left\n",
        "    cdef np.int64_t[:] R = right\n",
        "\n",
        "    cdef np.ndarray[np.int64_t, ndim=1] first      = np.empty(n_nodes, dtype=np.int64)\n",
        "    cdef np.ndarray[np.int64_t, ndim=1] last       = np.empty(n_nodes, dtype=np.int64)\n",
        "    cdef np.ndarray[np.int64_t, ndim=1] leaf_order = np.empty(m,       dtype=np.int64)\n",
        "    cdef np.ndarray[np.int64_t, ndim=1] pos        = np.empty(m,       dtype=np.int64)\n",
        "\n",
        "    cdef np.int64_t[:] first_v = first\n",
        "    cdef np.int64_t[:] last_v  = last\n",
        "    cdef np.int64_t[:] lo_v    = leaf_order\n",
        "    cdef np.int64_t[:] pos_v   = pos\n",
        "\n",
        "    cdef Py_ssize_t i\n",
        "    for i in range(n_nodes):\n",
        "        first_v[i] = -1\n",
        "        last_v[i]  = -1\n",
        "\n",
        "    # Pile itérative: (node, state) ; state=0 pre, 1 post\n",
        "    cdef np.ndarray[np.int64_t, ndim=1] stack_node = np.empty(n_nodes, dtype=np.int64)\n",
        "    cdef np.ndarray[np.int8_t,  ndim=1] stack_st   = np.empty(n_nodes, dtype=np.int8)\n",
        "    cdef np.int64_t[:] st_node = stack_node\n",
        "    cdef np.int8_t[:]  st_st   = stack_st\n",
        "\n",
        "    cdef Py_ssize_t sp = 0\n",
        "    cdef np.int64_t root = m + t - 1\n",
        "    st_node[sp] = root; st_st[sp] = 0; sp += 1\n",
        "\n",
        "    cdef Py_ssize_t k = 0\n",
        "    cdef np.int64_t x, state, child_idx, a, b, fa, fb, la, lb\n",
        "\n",
        "    while sp > 0:\n",
        "        sp -= 1\n",
        "        x = st_node[sp]\n",
        "        state = st_st[sp]\n",
        "\n",
        "        if x < m:\n",
        "            first_v[x] = k\n",
        "            last_v[x]  = k\n",
        "            lo_v[k]    = x\n",
        "            k += 1\n",
        "            continue\n",
        "\n",
        "        child_idx = x - m\n",
        "        if not (0 <= child_idx < t):\n",
        "            raise ValueError(\"Invalid internal node index\")\n",
        "\n",
        "        if state == 0:\n",
        "            # post-ordre: on empile soi (state=1), puis les enfants\n",
        "            st_node[sp] = x; st_st[sp] = 1; sp += 1\n",
        "            b = R[child_idx]\n",
        "            a = L[child_idx]\n",
        "            if a >= x or b >= x or a < 0 or b < 0:\n",
        "                raise ValueError(\"SciPy linkage convention violated: child >= parent\")\n",
        "            st_node[sp] = b; st_st[sp] = 0; sp += 1\n",
        "            st_node[sp] = a; st_st[sp] = 0; sp += 1\n",
        "        else:\n",
        "            a = L[child_idx]\n",
        "            b = R[child_idx]\n",
        "            fa = first_v[a]; fb = first_v[b]\n",
        "            la = last_v[a];  lb = last_v[b]\n",
        "            if fa == -1 or fb == -1:\n",
        "                raise ValueError(\"Invalid tree: child interval not computed\")\n",
        "            first_v[x] = _min_i64(fa, fb)\n",
        "            last_v[x]  = _max_i64(la, lb)\n",
        "\n",
        "    if k != m:\n",
        "        raise ValueError(\"Leaf DFS did not visit all leaves\")\n",
        "\n",
        "    for i in range(m):\n",
        "        pos_v[lo_v[i]] = i\n",
        "\n",
        "    return pos, first, last, leaf_order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E68cZdigcbkm",
      "metadata": {
        "id": "E68cZdigcbkm"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# #%run /content/drive/MyDrive/Colab_modules/custom_hdbscan.ipynb\n",
        "\n",
        "# import os, sys, pyximport\n",
        "# sys.path.append('/content/drive/MyDrive/Colab_modules')\n",
        "\n",
        "# # Compiler à l'import, et mettre les binaires dans Drive pour les réutiliser\n",
        "# build_dir = '/content/drive/MyDrive/Colab_modules/_build'\n",
        "# os.makedirs(build_dir, exist_ok=True)\n",
        "\n",
        "# pyximport.install(\n",
        "#     language_level=3,\n",
        "#     build_dir=build_dir,\n",
        "#     setup_args={\"include_dirs\": [np.get_include()]}  # nécessaire si cimport numpy\n",
        "# )\n",
        "\n",
        "# import custom_hdbscan  # compile si nécessaire, sinon réutilise le .so déjà construit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OfBe_AdOik5O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfBe_AdOik5O",
        "outputId": "a63f5860-aa5e-4d82-bf0c-1434ec3d9ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/plots.py:448: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  axis.set_ylabel('$\\lambda$ value')\n",
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n"
          ]
        }
      ],
      "source": [
        "# from custom_hdbscan import condense_tree, compute_stability, get_clusters\n",
        "\n",
        "import hdbscan\n",
        "from hdbscan._hdbscan_tree import condense_tree, compute_stability, get_clusters\n",
        "\n",
        "def tree_to_labels(single_linkage_tree, min_cluster_size=20, DBSCAN_threshold=None, cluster_selection_method=\"eom\", allow_single_cluster=True, match_reference_implementation=False, cluster_selection_epsilon=0.0, cluster_selection_persistence=0.0, max_cluster_size=0, cluster_selection_epsilon_max=float('inf')):\n",
        "    \"\"\"Converts a pretrained tree and cluster size into a\n",
        "    set of labels and probabilities.\n",
        "    \"\"\"\n",
        "    condensed_tree = condense_tree(single_linkage_tree, min_cluster_size)\n",
        "    stability_dict = compute_stability(condensed_tree)\n",
        "    # if single_linkage_tree.shape[0] > threshold_tree_size_for_EoM :\n",
        "    #     print(\"Z.shape[0] =\",single_linkage_tree.shape[0],\">\",threshold_tree_size_for_EoM,\" => cluster_selection_method='leaf'\")\n",
        "    #     labels, probabilities, stabilities = get_clusters(condensed_tree, stability_dict, \"leaf\", allow_single_cluster, match_reference_implementation, cluster_selection_epsilon, max_cluster_size, cluster_selection_epsilon_max)\n",
        "    # else :\n",
        "    if DBSCAN_threshold is None :\n",
        "        labels, probabilities, stabilities = get_clusters(condensed_tree, stability_dict, cluster_selection_method, allow_single_cluster, match_reference_implementation, cluster_selection_epsilon, max_cluster_size, cluster_selection_epsilon_max)\n",
        "    else :\n",
        "        # print(\"DBSCAN_threshold = \", DBSCAN_threshold)\n",
        "        labels, probabilities, stabilities = get_clusters(condensed_tree, stability_dict, \"leaf\", allow_single_cluster, match_reference_implementation, DBSCAN_threshold, max_cluster_size, DBSCAN_threshold)\n",
        "    return (labels, probabilities, stabilities, condensed_tree, single_linkage_tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f9mDGxemnC2",
      "metadata": {
        "id": "8f9mDGxemnC2"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import scipy.spatial\n",
        "\n",
        "convex_hull = scipy.spatial.ConvexHull # convex_hull_from_gudhi_delaunay #\n",
        "\n",
        "class Cell:\n",
        "    # list of vertices of the cell, which themselves are k-tuples of points\n",
        "    # k: the k for which this cell appears\n",
        "    def __init__(self, vertices, k):\n",
        "        self.vertices = vertices\n",
        "        self.k = k\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.vertices) + '[%d]' % self.k\n",
        "\n",
        "\n",
        "class OrderKDelaunay:\n",
        "    \"\"\"Order-k Delaunay mosaic for a set of points up to a given order k.\n",
        "\n",
        "    The constructor takes a list of input points (with a point represented as\n",
        "    a list of its coordinates) and an order. The point set can be in\n",
        "    Euclidean space of any dimension. Upon construction, the order-k Delaunay\n",
        "    mosaics of the points set from order 1 up to the specified order are\n",
        "    computed. The result can be accessed via the public attributes of the\n",
        "    OrderKDelaunay instance.\n",
        "\n",
        "    Public attributes:\n",
        "        diagrams_vertices:\n",
        "            List of vertex lists.\n",
        "            For each order-k Delaunay mosaic from 1 up to order, the list\n",
        "            of vertices where each vertex is a k-tuple of point indices.\n",
        "        diagrams_simplices:\n",
        "            List of simplex lists.\n",
        "            For each order-k Delaunay mosaic from 1 up to order,\n",
        "            the list of top-dimensional simplices of a triangulation of the\n",
        "            mosaic, where each simplex is a (d+1)-tuple of indices into the\n",
        "            corresponding vertex list.\n",
        "        diagrams_cells:\n",
        "            List of cell lists.\n",
        "            For each order-k Delaunay mosaic from 1 up to order,\n",
        "            the list of top-dimensional cells of the mosaic, where each cell\n",
        "            is a tuple of vertices (i.e. k-tuples of point indices) which spans\n",
        "            the cell (i.e. the convex hull is convex hull of the vertices).\n",
        "        diagrams_generations:\n",
        "            List of generation lists.\n",
        "            For each order-k Delaunay mosaic from 1 up to order,\n",
        "            the list of generations of the top-dimensional cells of the mosaic,\n",
        "            i.e. each cell from diagrams_cells is associated a generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, points, order, verbeux=False):\n",
        "        '''\n",
        "        Parameters:\n",
        "            points - list of points\n",
        "            order - order k up to which to compute the order-k Delaunay mosaics\n",
        "        '''\n",
        "        self.diagrams_vertices = []\n",
        "        self.diagrams_simplices = []\n",
        "        self.diagrams_cells = []\n",
        "        self.diagrams_generations = []\n",
        "        self.verbeux=verbeux\n",
        "\n",
        "        # Dimension of the ambient space.\n",
        "        self._dimension = len(points[0])\n",
        "        # Store each point with its magnitude as last coordinate, giving a\n",
        "        # a point in R^d+1, because we're using a lower convex hull to get\n",
        "        # the order-k cells from the vertex set.\n",
        "        self.lifts = [tuple(np.append(c, np.linalg.norm(c)**2))\n",
        "                      for c in points]\n",
        "\n",
        "        # Compute all the mosaics\n",
        "        self._compute_order_1()\n",
        "        for k in range(2, order + 1):\n",
        "            self._compute_order_k(k)\n",
        "\n",
        "    def _compute_order_1(self):\n",
        "        # Get first order Delaunay mosaic as lower convex hull of the lifts\n",
        "        chull = convex_hull(self.lifts) # scipy.spatial.ConvexHull(self.lifts) #, qhull_options='Qs')\n",
        "        if self.verbeux :\n",
        "            print(\"_compute_order_1 done\")\n",
        "        # chull.equations[i][dimension] < 0 means only taking the\n",
        "        # lower convex hull of the lifts\n",
        "        simplices = [chull.simplices[i] for i in range(len(chull.simplices))\n",
        "                     if chull.equations[i][self._dimension] < 0]\n",
        "\n",
        "        tupled_simplices = [[(vertex,) for vertex in simplex]\n",
        "                            for simplex in simplices]\n",
        "        self.diagrams_vertices.append([(i,) for i in range(len(self.lifts))])\n",
        "        self.diagrams_simplices.append(simplices)\n",
        "        self.diagrams_cells.append(tupled_simplices)\n",
        "        self.diagrams_generations.append([1] * len(simplices))\n",
        "        # Make a list of the order 1 cells.\n",
        "        # cell_queue will be used as the list of cells who haven't gone through\n",
        "        # all their barycentric polytopes yet.\n",
        "        self.cell_queue = list(zip(tupled_simplices, [1] * len(simplices)))\n",
        "        self.cell_queue = [Cell(*cell) for cell in self.cell_queue]\n",
        "\n",
        "    def _compute_order_k(self, k):\n",
        "        # Step 2.1: Compute the vertices and generation >= 2 cells of the\n",
        "        # order-k Delaunay mosaic.\n",
        "        new_nextgen_cells = []\n",
        "        new_generations = []\n",
        "        new_vertices = []\n",
        "        cell_queue_new = []\n",
        "        # Go over all cells whose cycle of barycentric polytopes we haven't\n",
        "        # completed yet.\n",
        "        for cell in self.cell_queue:\n",
        "            # k - cell.k is the generation of the new barycentric cell we get.\n",
        "            generation = k - cell.k\n",
        "            if generation < self._dimension:\n",
        "                # Take all (generation+1)-tuples of vertices\n",
        "                # --> list of gtuples of vertices (which are tuples of points)\n",
        "                gtuples = list(itertools.combinations(\n",
        "                      cell.vertices, generation + 1))\n",
        "                # For each of these tuples of vertices, we take the union of\n",
        "                # the vertices to get a new vertex.\n",
        "                # The outer 'sorted' is not necessary, but ensures output is\n",
        "                # entirely sorted, which is useful for testing and comparing.\n",
        "                nvs = sorted([tuple(sorted(set.union(*[set(vertex)\n",
        "                      for vertex in gtuple]))) for gtuple in gtuples])\n",
        "                # The number of points in each new vertex should be k+1,\n",
        "                # otherwise something went wrong.\n",
        "                new_nextgen_cells.append(nvs)\n",
        "                new_generations.append(generation + 1)\n",
        "                new_vertices += nvs\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "            # Once the generation is dimension - 1, we've cycled through all\n",
        "            # barycentric polytopes and thus don't need to add the cell to the\n",
        "            # queue again\n",
        "            if k - cell.k < self._dimension - 1:\n",
        "                cell_queue_new.append(cell)\n",
        "\n",
        "        # List of new vertices without repetitions.\n",
        "        new_vertices = list(set(new_vertices))\n",
        "        # For each tuple that we identified as vertex,\n",
        "        # compute the centroid of its lifts.\n",
        "        new_lifts = np.array([np.sum(\n",
        "              [self.lifts[i] for i in new_vertex], axis=0) / k\n",
        "                    for new_vertex in new_vertices])\n",
        "\n",
        "        chull = convex_hull(new_lifts) # scipy.spatial.ConvexHull(new_lifts) #, qhull_options='Qs')\n",
        "        if self.verbeux :\n",
        "            print(\"_compute_order_k done, k=\", k)\n",
        "        # Compute the simplices of the triangulated order-k Delaunay mosaic,\n",
        "        # which is the lower convex hull of these centroids.\n",
        "        # Each simplex is a tuple of integers, these integers are indices into\n",
        "        # new_vertices, which contains the k-tuples of original points which\n",
        "        # are vertices.\n",
        "        # (chull.equations[i][dimension] < 0 means only taking the\n",
        "        # lower convex hull of the lifts.)\n",
        "        simplices = [sorted(chull.simplices[i])\n",
        "              for i in range(len(chull.simplices))\n",
        "                    if chull.equations[i][self._dimension] < 0]\n",
        "\n",
        "        # Step 2.2: Compute the remaining cells of the order-k Delaunay mosaic\n",
        "        new_firstgen_cells = []\n",
        "        for simplex in simplices:\n",
        "            # Get the k-tuples that are the vertices of the simplex.\n",
        "            vertices = [set(new_vertices[i]) for i in simplex]\n",
        "            x_in = set.intersection(*vertices)\n",
        "            # Simplices are first generation if the intersection of their\n",
        "            # vertices is k-1.\n",
        "            if len(x_in) == k - 1:\n",
        "                # The outer 'sorted' is not necessary, but ensures the output\n",
        "                # is entirely sorted, which is useful for testing and comparing\n",
        "                cell_vertices = sorted([new_vertices[i] for i in simplex])\n",
        "                new_firstgen_cells.append(cell_vertices)\n",
        "                cell_queue_new.append(Cell(cell_vertices, k))\n",
        "\n",
        "        # Use our compiled queue for the next iteration\n",
        "        self.cell_queue = cell_queue_new\n",
        "\n",
        "        # Save the computed stuff\n",
        "        self.diagrams_vertices.append(new_vertices)\n",
        "        self.diagrams_simplices.append(simplices)\n",
        "        self.diagrams_cells.append(new_nextgen_cells + new_firstgen_cells)\n",
        "        self.diagrams_generations.append(\n",
        "            new_generations + [1] * len(new_firstgen_cells))\n",
        "\n",
        "\n",
        "\n",
        "# import matplotlib.colors as colors\n",
        "# import matplotlib.pyplot as plt\n",
        "# import mpl_toolkits.mplot3d as a3\n",
        "# import pylab\n",
        "\n",
        "# class Plotter:\n",
        "#     '''Abstract class for plotting order-k Delaunay mosaics.\n",
        "\n",
        "#     The actual plotting functionality is implemented in Plotter2D and\n",
        "#     Plotter3D. All classes use the constructor from Plotter, which\n",
        "#     defines the following public attributes for plotting settings:\n",
        "\n",
        "#     Public attributes:\n",
        "#         draw_input_points: True/False - Whether to draw the input point set.\n",
        "#         draw_labels: True/False - Whether to draw labels for the vertices and\n",
        "#                      input points.\n",
        "#         colors_cells: list of 3 matplotlib.colors - for first, second and\n",
        "#                       third generation cells.\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, points, orderk_delaunay):\n",
        "#         '''Initialize various drawing settings.\n",
        "\n",
        "#         These are public attributes and can be modified by the user.\n",
        "#         '''\n",
        "#         self._points = points\n",
        "#         self._orderk_delaunay = orderk_delaunay\n",
        "\n",
        "#         # Whether to draw a label for each vertex\n",
        "#         self.draw_labels = True\n",
        "\n",
        "#         # Whether to draw a label for each vertex\n",
        "#         self.draw_input_points = True\n",
        "\n",
        "#         # colors of the cells of each generation\n",
        "#         color_firstgen_cell = colors.rgb2hex((1.0, 0.0, 0.0))\n",
        "#         color_secondgen_cell = colors.rgb2hex((0.0, 0.0, 1.0))\n",
        "#         color_thirdgen_cell = colors.rgb2hex((1.0, 1.0, 0.0))\n",
        "#         self.colors_cells = [\n",
        "#               color_firstgen_cell, color_secondgen_cell, color_thirdgen_cell]\n",
        "\n",
        "#         # separator of the points indices that the vertex label consists of\n",
        "#         self._label_sep = ''\n",
        "#         if len(points) > 10:\n",
        "#             self._label_sep = ','\n",
        "\n",
        "#     def draw(self, order):\n",
        "#         '''Stub. Implemented by subclasses.'''\n",
        "#         pass\n",
        "\n",
        "\n",
        "# class Plotter2D(Plotter):\n",
        "\n",
        "#     def __init__(self, points, orderk_delaunay):\n",
        "#         super().__init__(points, orderk_delaunay)\n",
        "\n",
        "#     def draw(self, order):\n",
        "#         vertices = self._orderk_delaunay.diagrams_vertices[order-1]\n",
        "#         cells = self._orderk_delaunay.diagrams_cells[order-1]\n",
        "#         generations = self._orderk_delaunay.diagrams_generations[order-1]\n",
        "\n",
        "#         ax = plt.gca()\n",
        "#         ax.cla()\n",
        "#         # Draw cells stemming from barycentric subdivisions of older cells.\n",
        "#         # Either new_nextgen_cells or triangulated_cells is empty, depending on\n",
        "#         # whether 'triangulate' is True or False.\n",
        "#         for tri, gen in zip(cells, generations):\n",
        "#             vxs = []\n",
        "#             for vx in tri:\n",
        "#                 xs = [self._points[c][0] for c in vx]\n",
        "#                 ys = [self._points[c][1] for c in vx]\n",
        "#                 xcenter = sum(xs)/len(xs)\n",
        "#                 ycenter = sum(ys)/len(ys)\n",
        "#                 vxs.append([xcenter, ycenter])\n",
        "#             p = plt.Polygon(vxs, closed=True, fill=True,\n",
        "#                             color=self.colors_cells[gen-1], alpha = 0.4)\n",
        "#             ax.add_patch(p)\n",
        "#             for pair in itertools.combinations(vxs, 2):\n",
        "#                 pair = np.array(pair)\n",
        "#                 line = plt.Line2D(pair[:, 0], pair[:, 1],\n",
        "#                       color=self.colors_cells[gen-1], linewidth = 1.0)\n",
        "#                 ax.add_artist(line)\n",
        "\n",
        "#         # order-k points\n",
        "#         centroids = np.array([np.sum(\n",
        "#               [self._points[i] for i in vertex], axis=0)/order\n",
        "#                     for vertex in vertices])\n",
        "#         ax.plot(centroids[:,0], centroids[:,1], 'o', color=\"black\")\n",
        "#         if self.draw_labels:\n",
        "#             for c, v in zip(centroids, vertices):\n",
        "#                 ax.text(c[0], c[1], self._label_sep.join([str(pt) for pt in v]))\n",
        "#         # first-order points\n",
        "#         if self.draw_input_points:\n",
        "#             ax.plot(self._points[:,0], self._points[:,1], 'o', color='red')\n",
        "#             if self.draw_labels:\n",
        "#                 for i in range(len(self._points)):\n",
        "#                     ax.text(self._points[i][0], self._points[i][1], str(i))\n",
        "\n",
        "#         plt.show()\n",
        "\n",
        "\n",
        "# class Plotter3D(Plotter):\n",
        "\n",
        "#     def __init__(self, points, orderk_delaunay):\n",
        "#         super().__init__(points, orderk_delaunay)\n",
        "\n",
        "#         # Factor by how much to shrink simplices (helps visualization).\n",
        "#         self.shrinking_factor = 0.25\n",
        "\n",
        "#     def draw(self, order):\n",
        "#         vertices = self._orderk_delaunay.diagrams_vertices[order-1]\n",
        "#         cells = self._orderk_delaunay.diagrams_cells[order-1]\n",
        "#         generations = self._orderk_delaunay.diagrams_generations[order-1]\n",
        "\n",
        "#         ax = a3.Axes3D(pylab.figure())\n",
        "#         for cell, gen in zip(cells, generations):\n",
        "#             gvertices = []\n",
        "#             # compute the geometric vertices\n",
        "#             for gvertex in cell:\n",
        "#                 gvertices.append(np.mean(np.array(\n",
        "#                       [self._points[i][0:3] for i in gvertex]), axis=0))\n",
        "#             gvertices = np.array(gvertices)\n",
        "#             col = self.colors_cells[gen-1]\n",
        "#             faces = []\n",
        "#             cell_center = np.mean(gvertices, axis=0)\n",
        "\n",
        "#             # tetrahedron\n",
        "#             if len(cell) == 4:\n",
        "#                 for facet in itertools.combinations(gvertices, 3):\n",
        "#                     # geometric facet\n",
        "#                     gfacet = [gv[0:3]*(1-self.shrinking_factor) + \\\n",
        "#                           cell_center*(self.shrinking_factor) for gv in facet]\n",
        "#                     faces.append(gfacet)\n",
        "#             # octahedron\n",
        "#             elif len(cell) == 6:\n",
        "#                 for facet_ids in itertools.combinations(range(len(gvertices)), 3):\n",
        "#                     facet = [cell[i] for i in facet_ids]\n",
        "#                     gfacet = [gvertices[i] for i in facet_ids]\n",
        "#                     is_facet = True\n",
        "#                     # three vertices only span a facet if they pairwise differ\n",
        "#                     # in only one element.\n",
        "#                     for edge in itertools.combinations(facet, 2):\n",
        "#                         if len(set(edge[0]) & set(edge[1])) != order-1:\n",
        "#                             is_facet = False\n",
        "#                     if is_facet:\n",
        "#                         # geometric facet\n",
        "#                         gfacet = [gv[0:3]*(1-self.shrinking_factor) + \\\n",
        "#                               cell_center*(self.shrinking_factor)\n",
        "#                                     for gv in gfacet]\n",
        "#                         faces.append(gfacet)\n",
        "#             tri = a3.art3d.Poly3DCollection(faces, alpha=0.2)\n",
        "#             tri.set_color(col)\n",
        "#             tri.set_edgecolor('k')\n",
        "#             ax.add_collection3d(tri)\n",
        "#         # order-k points\n",
        "#         centroids = np.array([np.sum(\n",
        "#               [self._points[i] for i in vertex], axis=0)/order\n",
        "#                     for vertex in vertices])\n",
        "#         ax.scatter(centroids[:,0], centroids[:,1], centroids[:,2], 'o',\n",
        "#                    color=\"black\")\n",
        "#         if self.draw_labels:\n",
        "#             for c, v in zip(centroids, vertices):\n",
        "#                 ax.text(c[0], c[1], c[2], self._label_sep.join([str(pt) for pt in v]))\n",
        "#         # first-order points\n",
        "#         if self.draw_input_points:\n",
        "#             ax.scatter(self._points[:,0], self._points[:,1], self._points[:,2],\n",
        "#                   marker='o',color='red')\n",
        "#             if self.draw_labels:\n",
        "#                 for i in range(len(self._points)):\n",
        "#                     ax.text(self._points[i][0], self._points[i][1],\n",
        "#                             self._points[i][2], str(i))\n",
        "\n",
        "#         plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QyjSQjveSUc0",
      "metadata": {
        "id": "QyjSQjveSUc0"
      },
      "outputs": [],
      "source": [
        "# # # 2D example point set\n",
        "# # points = np.array([[156.006,705.854],\n",
        "# #         [215.257,732.63], [283.108,707.272], [244.042,670.948],\n",
        "# #         [366.035,687.396], [331.768,625.715], [337.936,559.92],\n",
        "# #         [249.525,582.537], [187.638,556.13], [165.912,631.197]])\n",
        "\n",
        "# # 3D example point set\n",
        "# # points = np.array([(0,0,0), (0,4,4), (4,4,0), (4,0,4), (-10,2,2)])\n",
        "# rng = np.random.default_rng(7)\n",
        "# points = rng.random((10, 2))\n",
        "\n",
        "# # the order k up to which to compute the order-k Delaunay diagram\n",
        "# order = 3\n",
        "# # Whether to print the cells of all the complexes\n",
        "# print_output = True\n",
        "# # Whether to draw all the order-k Delaunay mosaics\n",
        "# draw_output = True\n",
        "\n",
        "# # Compute the order-k Delaunay mosaics\n",
        "# orderk_delaunay = OrderKDelaunay(points, order)\n",
        "\n",
        "# # Initialize appropriate plotter for drawing the mosaics.\n",
        "# dimension = len(points[0])\n",
        "# if dimension == 2:\n",
        "#     plotter = Plotter2D(points, orderk_delaunay)\n",
        "# elif dimension == 3:\n",
        "#     plotter = Plotter3D(points, orderk_delaunay)\n",
        "# else:\n",
        "#     # Stub that doesn't draw anything.\n",
        "#     plotter = Plotter(points, orderk_delaunay)\n",
        "\n",
        "# for k in range(1, order+1):\n",
        "#     if draw_output:\n",
        "#         plotter.draw(k)\n",
        "\n",
        "#     if print_output:\n",
        "#         cells = orderk_delaunay.diagrams_cells[k-1]\n",
        "#         # Output all the cells.\n",
        "#         print(\"Order {}. Number of cells: {}\".format(\n",
        "#                 len(cells[0][0]), len(cells)))\n",
        "#         for cell in sorted(cells):\n",
        "#             print(cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oFIZaNYGulDZ",
      "metadata": {
        "id": "oFIZaNYGulDZ"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi\n",
        "# !nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eH0nSLteumRy",
      "metadata": {
        "id": "eH0nSLteumRy"
      },
      "outputs": [],
      "source": [
        "# !apt-get update\n",
        "# !apt-get install -y build-essential git cmake nvidia-cuda-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sCMvHd2PumUn",
      "metadata": {
        "id": "sCMvHd2PumUn"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/Ludwig-H/gDel3D.git\n",
        "# %cd gDel3D\n",
        "# !cmake -S . -B build\n",
        "# !cmake --build build -j8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Jrn8aTdXe7I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jrn8aTdXe7I",
        "outputId": "4ec95ef6-d20c-46c1-d79b-3f489bba3c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [81.0 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,307 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,804 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,577 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,808 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,690 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,371 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,609 kB]\n",
            "Fetched 33.9 MB in 4s (7,760 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libtbb-dev is already the newest version (2021.5.0-7ubuntu2).\n",
            "libtbb-dev set to manually installed.\n",
            "libtbbmalloc2 is already the newest version (2021.5.0-7ubuntu2).\n",
            "libtbbmalloc2 set to manually installed.\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "The following additional packages will be installed:\n",
            "  libgmpxx4ldbl\n",
            "Suggested packages:\n",
            "  libmpfi-dev libntl-dev libeigen3-doc libmpfrc++-dev gmp-doc libgmp10-doc\n",
            "  libmpfr-doc\n",
            "The following NEW packages will be installed:\n",
            "  libcgal-dev libeigen3-dev libgmp-dev libgmpxx4ldbl libmpfr-dev\n",
            "0 upgraded, 5 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 6,457 kB of archives.\n",
            "After this operation, 52.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgmpxx4ldbl amd64 2:6.2.1+dfsg-3ubuntu1 [9,580 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgmp-dev amd64 2:6.2.1+dfsg-3ubuntu1 [337 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmpfr-dev amd64 4.1.0-3build3 [271 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcgal-dev amd64 5.4-1 [4,784 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeigen3-dev all 3.4.0-2ubuntu2 [1,056 kB]\n",
            "Fetched 6,457 kB in 3s (1,874 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgmpxx4ldbl:amd64.\n",
            "(Reading database ... 126441 files and directories currently installed.)\n",
            "Preparing to unpack .../libgmpxx4ldbl_2%3a6.2.1+dfsg-3ubuntu1_amd64.deb ...\n",
            "Unpacking libgmpxx4ldbl:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\n",
            "Selecting previously unselected package libgmp-dev:amd64.\n",
            "Preparing to unpack .../libgmp-dev_2%3a6.2.1+dfsg-3ubuntu1_amd64.deb ...\n",
            "Unpacking libgmp-dev:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\n",
            "Selecting previously unselected package libmpfr-dev:amd64.\n",
            "Preparing to unpack .../libmpfr-dev_4.1.0-3build3_amd64.deb ...\n",
            "Unpacking libmpfr-dev:amd64 (4.1.0-3build3) ...\n",
            "Selecting previously unselected package libcgal-dev:amd64.\n",
            "Preparing to unpack .../libcgal-dev_5.4-1_amd64.deb ...\n",
            "Unpacking libcgal-dev:amd64 (5.4-1) ...\n",
            "Selecting previously unselected package libeigen3-dev.\n",
            "Preparing to unpack .../libeigen3-dev_3.4.0-2ubuntu2_all.deb ...\n",
            "Unpacking libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Setting up libgmpxx4ldbl:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\n",
            "Setting up libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Setting up libgmp-dev:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\n",
            "Setting up libmpfr-dev:amd64 (4.1.0-3build3) ...\n",
            "Setting up libcgal-dev:amd64 (5.4-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y build-essential cmake libcgal-dev libtbb-dev libtbbmalloc2 libgmp-dev libmpfr-dev libeigen3-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kiFbg_bqXB-N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiFbg_bqXB-N",
        "outputId": "8ac797b7-7e7c-4f51-d542-ad4ff0b2064a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EdgesCGALWeightedDelaunay3D'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 21 (delta 7), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (21/21), 31.66 KiB | 900.00 KiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "/content/EdgesCGALWeightedDelaunay3D\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:92 (message):\n",
            "  CGAL_DATA_DIR cannot be deduced, set the variable CGAL_DATA_DIR to set the\n",
            "  default value of CGAL::data_file_path()\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:15 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Using header-only CGAL\n",
            "-- Targetting Unix Makefiles\n",
            "-- Using /usr/bin/c++ compiler.\n",
            "-- Found GMP: /usr/lib/x86_64-linux-gnu/libgmp.so\n",
            "-- Found MPFR: /usr/lib/x86_64-linux-gnu/libmpfr.so\n",
            "-- Boost include dirs: /usr/include\n",
            "-- Boost libraries:    \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Using gcc version 4 or later. Adding -frounding-math\n",
            "-- Configuring done (0.6s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/EdgesCGALWeightedDelaunay3D/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/EdgesCGALWeightedDelaunay3D.dir/src/EdgesCGALWeightedDelaunay3D.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable EdgesCGALWeightedDelaunay3D\u001b[0m\n",
            "[100%] Built target EdgesCGALWeightedDelaunay3D\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ludwig-H/EdgesCGALWeightedDelaunay3D.git\n",
        "%cd EdgesCGALWeightedDelaunay3D\n",
        "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AWo9oXd0cdjv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWo9oXd0cdjv",
        "outputId": "6c3a5bd5-8818-431d-8a02-4182f02f3597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EdgesCGALWeightedDelaunay2D'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 27 (delta 9), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (27/27), 24.42 KiB | 735.00 KiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "/content/EdgesCGALWeightedDelaunay2D\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:92 (message):\n",
            "  CGAL_DATA_DIR cannot be deduced, set the variable CGAL_DATA_DIR to set the\n",
            "  default value of CGAL::data_file_path()\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:6 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Using header-only CGAL\n",
            "-- Targetting Unix Makefiles\n",
            "-- Using /usr/bin/c++ compiler.\n",
            "-- Found GMP: /usr/lib/x86_64-linux-gnu/libgmp.so\n",
            "-- Found MPFR: /usr/lib/x86_64-linux-gnu/libmpfr.so\n",
            "\u001b[33mCMake Warning (dev) at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupBoost.cmake:20 (find_package):\n",
            "  Policy CMP0167 is not set: The FindBoost module is removed.  Run \"cmake\n",
            "  --help-policy CMP0167\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupCGALDependencies.cmake:47 (include)\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:153 (include)\n",
            "  CMakeLists.txt:6 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.48\")\n",
            "-- Boost include dirs: /usr/include\n",
            "-- Boost libraries:    \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Using gcc version 4 or later. Adding -frounding-math\n",
            "-- Configuring done (0.3s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/EdgesCGALWeightedDelaunay2D/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/EdgesCGALWeightedDelaunay2D.dir/src/EdgesCGALWeightedDelaunay2D.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable EdgesCGALWeightedDelaunay2D\u001b[0m\n",
            "[100%] Built target EdgesCGALWeightedDelaunay2D\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ludwig-H/EdgesCGALWeightedDelaunay2D.git\n",
        "%cd EdgesCGALWeightedDelaunay2D\n",
        "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i_jx3u18nd03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_jx3u18nd03",
        "outputId": "a9418f02-db47-4d26-db70-3dffcfc441c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EdgesCGALWeightedDelaunayND'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 25 (delta 9), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (25/25), 28.42 KiB | 786.00 KiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "/content/EdgesCGALWeightedDelaunayND\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:92 (message):\n",
            "  CGAL_DATA_DIR cannot be deduced, set the variable CGAL_DATA_DIR to set the\n",
            "  default value of CGAL::data_file_path()\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:7 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Using header-only CGAL\n",
            "-- Targetting Unix Makefiles\n",
            "-- Using /usr/bin/c++ compiler.\n",
            "-- Found GMP: /usr/lib/x86_64-linux-gnu/libgmp.so\n",
            "-- Found MPFR: /usr/lib/x86_64-linux-gnu/libmpfr.so\n",
            "\u001b[33mCMake Warning (dev) at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupBoost.cmake:20 (find_package):\n",
            "  Policy CMP0167 is not set: The FindBoost module is removed.  Run \"cmake\n",
            "  --help-policy CMP0167\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupCGALDependencies.cmake:47 (include)\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:153 (include)\n",
            "  CMakeLists.txt:7 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.48\")\n",
            "-- Boost include dirs: /usr/include\n",
            "-- Boost libraries:    \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Using gcc version 4 or later. Adding -frounding-math\n",
            "-- Found Eigen3: /usr/include/eigen3 (found suitable version \"3.4.0\", minimum required is \"3.3\")\n",
            "-- Configuring done (0.3s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/EdgesCGALWeightedDelaunayND/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/EdgesCGALWeightedDelaunayND.dir/src/EdgesCGALWeightedDelaunayND.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable EdgesCGALWeightedDelaunayND\u001b[0m\n",
            "[100%] Built target EdgesCGALWeightedDelaunayND\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ludwig-H/EdgesCGALWeightedDelaunayND.git\n",
        "%cd EdgesCGALWeightedDelaunayND\n",
        "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7otzs5ZaXnq9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7otzs5ZaXnq9",
        "outputId": "be0f97e3-b6e2-4215-8785-275094e47bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EdgesCGALDelaunay3D'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 21 (delta 7), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (21/21), 25.64 KiB | 729.00 KiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "/content/EdgesCGALDelaunay3D\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:92 (message):\n",
            "  CGAL_DATA_DIR cannot be deduced, set the variable CGAL_DATA_DIR to set the\n",
            "  default value of CGAL::data_file_path()\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:14 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Using header-only CGAL\n",
            "-- Targetting Unix Makefiles\n",
            "-- Using /usr/bin/c++ compiler.\n",
            "-- Found GMP: /usr/lib/x86_64-linux-gnu/libgmp.so\n",
            "-- Found MPFR: /usr/lib/x86_64-linux-gnu/libmpfr.so\n",
            "-- Boost include dirs: /usr/include\n",
            "-- Boost libraries:    \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Using gcc version 4 or later. Adding -frounding-math\n",
            "-- Configuring done (0.3s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/EdgesCGALDelaunay3D/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/EdgesCGALDelaunay3D.dir/src/EdgesCGALDelaunay3D.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable EdgesCGALDelaunay3D\u001b[0m\n",
            "[100%] Built target EdgesCGALDelaunay3D\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ludwig-H/EdgesCGALDelaunay3D.git\n",
        "%cd EdgesCGALDelaunay3D\n",
        "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E4tIwGgUchAA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4tIwGgUchAA",
        "outputId": "eac5ec08-d5db-4be8-b23b-f27ccdc53c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EdgesCGALDelaunay2D'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 26 (delta 10), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (26/26), 16.11 KiB | 471.00 KiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "/content/EdgesCGALDelaunay2D\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:92 (message):\n",
            "  CGAL_DATA_DIR cannot be deduced, set the variable CGAL_DATA_DIR to set the\n",
            "  default value of CGAL::data_file_path()\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:6 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Using header-only CGAL\n",
            "-- Targetting Unix Makefiles\n",
            "-- Using /usr/bin/c++ compiler.\n",
            "-- Found GMP: /usr/lib/x86_64-linux-gnu/libgmp.so\n",
            "-- Found MPFR: /usr/lib/x86_64-linux-gnu/libmpfr.so\n",
            "\u001b[33mCMake Warning (dev) at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupBoost.cmake:20 (find_package):\n",
            "  Policy CMP0167 is not set: The FindBoost module is removed.  Run \"cmake\n",
            "  --help-policy CMP0167\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupCGALDependencies.cmake:47 (include)\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:153 (include)\n",
            "  CMakeLists.txt:6 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.48\")\n",
            "-- Boost include dirs: /usr/include\n",
            "-- Boost libraries:    \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Using gcc version 4 or later. Adding -frounding-math\n",
            "-- Configuring done (0.3s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/EdgesCGALDelaunay2D/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/EdgesCGALDelaunay2D.dir/src/EdgesCGALDelaunay2D.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable EdgesCGALDelaunay2D\u001b[0m\n",
            "[100%] Built target EdgesCGALDelaunay2D\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ludwig-H/EdgesCGALDelaunay2D.git\n",
        "%cd EdgesCGALDelaunay2D\n",
        "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jPVNQc_RnicU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPVNQc_RnicU",
        "outputId": "938cd727-686c-4d8f-d264-eea5d847afe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EdgesCGALDelaunayND'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 20 (delta 7), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (20/20), 36.60 KiB | 961.00 KiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "/content/EdgesCGALDelaunayND\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:92 (message):\n",
            "  CGAL_DATA_DIR cannot be deduced, set the variable CGAL_DATA_DIR to set the\n",
            "  default value of CGAL::data_file_path()\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:12 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Using header-only CGAL\n",
            "-- Targetting Unix Makefiles\n",
            "-- Using /usr/bin/c++ compiler.\n",
            "-- Found GMP: /usr/lib/x86_64-linux-gnu/libgmp.so\n",
            "-- Found MPFR: /usr/lib/x86_64-linux-gnu/libmpfr.so\n",
            "\u001b[33mCMake Warning (dev) at /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupBoost.cmake:20 (find_package):\n",
            "  Policy CMP0167 is not set: The FindBoost module is removed.  Run \"cmake\n",
            "  --help-policy CMP0167\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGAL_SetupCGALDependencies.cmake:47 (include)\n",
            "  /usr/lib/x86_64-linux-gnu/cmake/CGAL/CGALConfig.cmake:153 (include)\n",
            "  CMakeLists.txt:12 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.48\")\n",
            "-- Boost include dirs: /usr/include\n",
            "-- Boost libraries:    \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Using gcc version 4 or later. Adding -frounding-math\n",
            "-- TBB found: enabling 3D parallel fast-path\n",
            "-- Configuring done (0.4s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/EdgesCGALDelaunayND/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/EdgesCGALDelaunayND.dir/src/EdgesCGALDelaunayND.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable EdgesCGALDelaunayND\u001b[0m\n",
            "[100%] Built target EdgesCGALDelaunayND\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ludwig-H/EdgesCGALDelaunayND.git\n",
        "%cd EdgesCGALDelaunayND\n",
        "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yf84w8_5cLj1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf84w8_5cLj1",
        "outputId": "32e381b6-6c49-4286-8352-cf4107e99dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gudhi\n",
            "  Downloading gudhi-3.11.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from gudhi) (2.0.2)\n",
            "Downloading gudhi-3.11.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gudhi\n",
            "Successfully installed gudhi-3.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gudhi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FbxDkBRTJ7nZ",
      "metadata": {
        "id": "FbxDkBRTJ7nZ"
      },
      "outputs": [],
      "source": [
        "# import gudhi\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "def _unique_sorted_rows(arr: np.ndarray, *, sort_rows: bool = False) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Déduplique des lignes d'entiers en supprimant les doublons (ensemble = ligne),\n",
        "    et renvoie les lignes triées lexicographiquement.\n",
        "    - Forçage dtype int64\n",
        "    - Optionnellement trie chaque ligne (utile si l'amont ne garantit pas ordre croissant)\n",
        "    - Tri global lexicographique\n",
        "    - Filtre les doublons adjacents\n",
        "    - Retourne un tableau C-contigu\n",
        "\n",
        "    Paramètres\n",
        "    ----------\n",
        "    arr : (m, k) array-like d'entiers\n",
        "\n",
        "    Retour\n",
        "    ------\n",
        "    (u, k) np.ndarray[int64]\n",
        "    \"\"\"\n",
        "    a = np.asarray(arr, dtype=np.int64)\n",
        "    if a.ndim != 2:\n",
        "        raise ValueError(f\"_unique_sorted_rows attend un 2D array, reçu {a.shape} / ndim={a.ndim}\")\n",
        "    m, k = a.shape\n",
        "    if m == 0:\n",
        "        return a.reshape(0, k)\n",
        "\n",
        "    # # S'assurer que chaque ligne est strictement croissante si nécessaire\n",
        "    if sort_rows:\n",
        "        # mergesort est stable; pas vital ici, mais ça garde un comportement prévisible\n",
        "        a = np.sort(a, axis=1, kind=\"mergesort\")\n",
        "\n",
        "        # Tri lexicographique des lignes: dernière colonne = clé primaire\n",
        "        # a.T[::-1] produit la liste des clés de lexsort dans le bon ordre\n",
        "        order = np.lexsort(a.T[::-1])\n",
        "        s = np.ascontiguousarray(a[order])  # C-contigu pour la suite\n",
        "    else :\n",
        "        s = np.ascontiguousarray(a)\n",
        "\n",
        "    # Garde la première ligne de chaque run de lignes identiques\n",
        "    keep = np.ones(m, dtype=bool)\n",
        "    if m > 1:\n",
        "        keep[1:] = np.any(s[1:] != s[:-1], axis=1)\n",
        "\n",
        "    return s[keep]\n",
        "\n",
        "\n",
        "\n",
        "def _edges_from_weighted_delaunay(points: np.ndarray, weights: np.ndarray = None, precision: str = \"safe\") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Graphe (1-squelette) de la Delaunay pondérée, pour points en R^d.\n",
        "    Retourne un tableau (e, 2) int64 avec i<j, dédoublonné.\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    # w_points = np.column_stack((points, weights))   # shape (n, 4)\n",
        "    N,d = points.shape\n",
        "    root_CGAL = \"/content\"\n",
        "    input_file = './my_point_cloud'+str(N)+'.npy'\n",
        "    input_file_weights = './my_weights'+str(N)+'.npy'\n",
        "    output_file = \"./sortie\"+str(N)+\".npy\"\n",
        "    np.save(input_file, points)\n",
        "    if weights is not None:\n",
        "        np.save(input_file_weights, weights)\n",
        "    if d == 3 :\n",
        "        if weights is not None:\n",
        "            command = [root_CGAL+\"/EdgesCGALWeightedDelaunay3D/build/EdgesCGALWeightedDelaunay3D\", input_file, input_file_weights, output_file]\n",
        "        else :\n",
        "            command = [root_CGAL+\"/EdgesCGALDelaunay3D/build/EdgesCGALDelaunay3D\", input_file, output_file]\n",
        "    elif d == 2 :\n",
        "        if weights is not None:\n",
        "            command = [root_CGAL+\"/EdgesCGALWeightedDelaunay2D/build/EdgesCGALWeightedDelaunay2D\", input_file, input_file_weights, output_file]\n",
        "        else :\n",
        "            command = [root_CGAL+\"/EdgesCGALDelaunay2D/build/EdgesCGALDelaunay2D\", input_file, output_file]\n",
        "    else :\n",
        "        if weights is not None :\n",
        "            command = [root_CGAL+\"/EdgesCGALWeightedDelaunayND/build/EdgesCGALWeightedDelaunayND\", input_file, input_file_weights, output_file]\n",
        "        else :\n",
        "            command = [root_CGAL+\"/EdgesCGALDelaunayND/build/EdgesCGALDelaunayND\", input_file, output_file]\n",
        "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "\n",
        "    edges_list = np.load(output_file).tolist()\n",
        "\n",
        "    os.remove(input_file)\n",
        "    if weights is not None:\n",
        "        os.remove(input_file_weights)\n",
        "    os.remove(output_file)\n",
        "\n",
        "    return edges_list\n",
        "\n",
        "\n",
        "\n",
        "def _build_all_keys(combos: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Aplati toutes les clés (k-1)-aires de tous les k-uplets.\n",
        "    combos: (m,k) int64 trié par ligne.\n",
        "    Retourne:\n",
        "      keys:    (m*k, k-1) int64  — toutes les clés, une par (combo,position)\n",
        "      parents: (m*k,)     int64  — index du combo parent pour chaque clé\n",
        "    \"\"\"\n",
        "    m, k = combos.shape\n",
        "    if k < 2:\n",
        "        return np.empty((0, 0), dtype=np.int64), np.empty((0,), dtype=np.int64)\n",
        "    keys_blocks = []\n",
        "    parents_blocks = []\n",
        "    rng = np.arange(m, dtype=np.int64)\n",
        "    for r in range(k):\n",
        "        # garder toutes colonnes sauf r\n",
        "        keep = [c for c in range(k) if c != r]\n",
        "        keys_blocks.append(combos[:, keep])\n",
        "        parents_blocks.append(rng)  # chaque ligne de ce bloc vient de combo i\n",
        "    keys = np.vstack(keys_blocks).astype(np.int64, copy=False)\n",
        "    parents = np.concatenate(parents_blocks).astype(np.int64, copy=False)\n",
        "    return keys, parents\n",
        "\n",
        "# def orderk_delaunay3(M: np.ndarray, K: int, precision: str = \"safe\", verbeux:bool = False) -> list[list[int]]:\n",
        "#     \"\"\"\n",
        "#     Variante locale optimisée: une seule passe de tri pour regrouper par toutes les clés (k-1)-aires.\n",
        "#     - K=1: Delaunay non pondérée sur M, retourne les arêtes [i,j].\n",
        "#     - K>=2: barycentres/poids pour tous les k-uplets, puis pour chaque clé (unique)\n",
        "#             on triangule localement une seule fois et on forme les (k+1)-uplets adjacents.\n",
        "#     \"\"\"\n",
        "#     M = np.asarray(M, dtype=np.float64)\n",
        "#     if M.ndim != 2 or M.shape[1] < 1:\n",
        "#         raise ValueError(f\"M doit être (n,d) avec d>=1, reçu {M.shape}\")\n",
        "#     if K < 1:\n",
        "#         raise ValueError(\"K doit être >= 1\")\n",
        "#     n = M.shape[0]\n",
        "#     if n < 2:\n",
        "#         return []\n",
        "\n",
        "#     # Pré-calcul ||p_i||^2\n",
        "#     s2_all = (M * M).sum(axis=1)\n",
        "\n",
        "#     # K=1: graphe Delaunay (poids nuls)\n",
        "#     edges = _edges_from_weighted_delaunay(M, np.zeros(n, dtype=np.float64), precision)\n",
        "#     prev = edges  # (m,2) int64\n",
        "\n",
        "#     if verbeux :\n",
        "#         print(\"orderk_delaunay k = 1\")\n",
        "\n",
        "#     if K == 1:\n",
        "#         return prev.tolist()\n",
        "\n",
        "#     for k in range(2, K + 1):\n",
        "#         if prev.shape[0] < 2:\n",
        "#             return []\n",
        "\n",
        "#         # k-uplets uniques\n",
        "#         combos = _unique_sorted_rows(prev.astype(np.int64, copy=False))\n",
        "#         combos = np.ascontiguousarray(combos, dtype=np.int64)\n",
        "\n",
        "#         # Barycentres/poids pour tous les combos\n",
        "#         Q = np.empty((combos.shape[0], M.shape[1]), dtype=np.float64)\n",
        "#         w = np.empty((combos.shape[0],), dtype=np.float64)\n",
        "#         bary_weight_batch(M, s2_all, combos, Q, w)\n",
        "\n",
        "#         # Une seule passe: construire toutes les clés (k-1)-aires et les trier une fois\n",
        "#         keys, parents = _build_all_keys(combos)\n",
        "#         if keys.size == 0:\n",
        "#             return []\n",
        "\n",
        "#         # Tri lexicographique des clés\n",
        "#         key_cols = [keys[:, c] for c in range(keys.shape[1] - 1, -1, -1)]\n",
        "#         order = np.lexsort(key_cols)\n",
        "#         keys_sorted = keys[order]\n",
        "#         parents_sorted = parents[order]\n",
        "\n",
        "#         # Délimitation des runs de clés égales\n",
        "#         start = np.ones(keys_sorted.shape[0], dtype=bool)\n",
        "#         start[1:] = np.any(keys_sorted[1:] != keys_sorted[:-1], axis=1)\n",
        "#         run_starts = np.flatnonzero(start)\n",
        "#         run_ends = np.r_[run_starts[1:], keys_sorted.shape[0]]\n",
        "\n",
        "#         next_candidates = []\n",
        "#         out_u = np.empty(k + 1, dtype=np.int64)\n",
        "\n",
        "#         # Une triangulation pondérée par clé unique\n",
        "#         for a, b in zip(run_starts, run_ends):\n",
        "#             L = b - a\n",
        "#             if L < 2:\n",
        "#                 continue\n",
        "#             loc = parents_sorted[a:b]                  # indices des combos dans cette étoile\n",
        "#             # loc = np.unique(loc)                       # au cas où\n",
        "#             if loc.size < 2:\n",
        "#                 continue\n",
        "\n",
        "#             e_local = _edges_from_weighted_delaunay(Q[loc], w[loc], precision)\n",
        "#             if e_local.size == 0:\n",
        "#                 continue\n",
        "\n",
        "#             for i, j in e_local:\n",
        "#                 A = combos[loc[i]]\n",
        "#                 B = combos[loc[j]]\n",
        "#                 if union_if_adjacent_int(A, B, out_u):\n",
        "#                     next_candidates.append(out_u.copy())\n",
        "\n",
        "#         if not next_candidates:\n",
        "#             return []\n",
        "\n",
        "#         prev = _unique_sorted_rows(np.asarray(next_candidates, dtype=np.int64))\n",
        "#         prev = np.ascontiguousarray(prev, dtype=np.int64)\n",
        "#         if verbeux :\n",
        "#             print(f\"orderk_delaunay k = {k}\")\n",
        "\n",
        "#     return prev.tolist()\n",
        "\n",
        "# def _sparsify_with_indices(Q: np.ndarray, epsilon: float,\n",
        "#                            *, rtol: float = 0.0, atol: float = 0.0) -> tuple[np.ndarray, np.ndarray]:\n",
        "#     \"\"\"\n",
        "#     Appelle gudhi.subsampling.sparsify_point_set(Q, epsilon) qui NE renvoie pas les indices.\n",
        "#     On reconstruit les indices en supposant que l’ordre des points est préservé.\n",
        "#     Repli robuste si le scan échoue (doublons / allclose).\n",
        "#     Retourne (Q_sub, idx) où idx indexe Q.\n",
        "#     \"\"\"\n",
        "#     if epsilon <= 0.0 or Q.shape[0] < 3:\n",
        "#         return Q, np.arange(Q.shape[0], dtype=np.int64)\n",
        "\n",
        "#     # Gudhi attend une liste Python\n",
        "#     Q_sub_list = gudhi.subsampling.sparsify_point_set(Q.tolist(), min_squared_dist=epsilon**2)\n",
        "#     if len(Q_sub_list) == 0:\n",
        "#         return Q[:0], np.empty((0,), dtype=np.int64)\n",
        "\n",
        "#     Q_sub = np.asarray(Q_sub_list, dtype=np.float64)\n",
        "#     # 1) Chemin rapide: deux-pointeurs, égalité stricte, ordre préservé\n",
        "#     idx = []\n",
        "#     j = 0\n",
        "#     for i in range(Q.shape[0]):\n",
        "#         if j >= Q_sub.shape[0]:\n",
        "#             break\n",
        "#         if np.array_equal(Q[i], Q_sub[j]):\n",
        "#             idx.append(i)\n",
        "#             j += 1\n",
        "#     if j == Q_sub.shape[0]:\n",
        "#         return Q[idx], np.asarray(idx, dtype=np.int64)\n",
        "\n",
        "#     # 2) Repli tolérant: deux-pointeurs avec allclose\n",
        "#     idx = []\n",
        "#     j = 0\n",
        "#     for i in range(Q.shape[0]):\n",
        "#         if j >= Q_sub.shape[0]:\n",
        "#             break\n",
        "#         if np.allclose(Q[i], Q_sub[j], rtol=rtol, atol=atol):\n",
        "#             idx.append(i)\n",
        "#             j += 1\n",
        "#     if j == Q_sub.shape[0]:\n",
        "#         return Q[idx], np.asarray(idx, dtype=np.int64)\n",
        "\n",
        "#     # 3) Repli robuste: dictionnaire de listes de positions (gère les doublons)\n",
        "#     #    On quantize pour clés stables si besoin.\n",
        "#     key = tuple\n",
        "#     def _key(v: np.ndarray) -> tuple:\n",
        "#         return tuple(v.tolist())\n",
        "\n",
        "#     pos = {}\n",
        "#     for i in range(Q.shape[0]):\n",
        "#         k = _key(Q[i])\n",
        "#         pos.setdefault(k, []).append(i)\n",
        "#     idx = []\n",
        "#     for row in Q_sub:\n",
        "#         k = _key(np.asarray(row, dtype=np.float64))\n",
        "#         if k not in pos or not pos[k]:\n",
        "#             raise RuntimeError(\"Impossible de reconstituer les indices après sparsification. \"\n",
        "#                                \"Hypothèse d'ordre préservé probablement violée.\")\n",
        "#         idx.append(pos[k].pop(0))\n",
        "#     idx = np.asarray(idx, dtype=np.int64)\n",
        "#     return Q[idx], idx\n",
        "\n",
        "\n",
        "def orderk_delaunay3(M: np.ndarray, K: int, precision: str = \"safe\", verbeux: bool = False) -> list[list[int]]:\n",
        "    \"\"\"\n",
        "    Version globale avec sparsification (Gudhi) avant chaque Delaunay pondérée.\n",
        "    Échantillonnage contrôlé par epsilon *= ||quantile_95(M) - quantile_05(M)||.\n",
        "\n",
        "      - K=1 : Delaunay non pondérée sur M → arêtes [i,j].\n",
        "      - K>=2 : calcule (barycentres, poids) pour TOUS les k-uplets,\n",
        "               SPARSIFIE (Q, w) et 'combos' de manière cohérente,\n",
        "               Delaunay pondérée globale sur l'échantillon,\n",
        "               puis ne garde que les unions de taille k+1.\n",
        "\n",
        "    Retour: liste de k-uplets (indices int) triés.\n",
        "    \"\"\"\n",
        "    M = np.ascontiguousarray(M, dtype=np.float64)\n",
        "    if M.ndim != 2 or M.shape[1] < 1:\n",
        "        raise ValueError(f\"M doit être (n,d) avec d>=1, reçu {M.shape}\")\n",
        "    if K < 1:\n",
        "        raise ValueError(\"K doit être >= 1\")\n",
        "    n, d = M.shape\n",
        "    if n < 2:\n",
        "        return []\n",
        "\n",
        "    # Pré-calcul ||p_i||^2 (option 2 que tu as retenue)\n",
        "    s2_all = (M * M).sum(axis=1)\n",
        "\n",
        "    # # Échelle epsilon basée sur M: ||q95 - q05||\n",
        "    # q05 = np.quantile(M, 0.05, axis=0)\n",
        "    # q95 = np.quantile(M, 0.95, axis=0)\n",
        "    # epsilon = 0.000001*float(np.linalg.norm(q95 - q05))\n",
        "    # if verbeux:\n",
        "    #     print(f\"epsilon (sparsify) = {epsilon:.6g}\")\n",
        "\n",
        "    # K=1 : graphe Delaunay non pondéré\n",
        "    prev = _edges_from_weighted_delaunay(M, precision=precision)\n",
        "    if verbeux:\n",
        "        print(\"orderk_delaunay k = 1\")\n",
        "    if K == 1:\n",
        "        return prev\n",
        "\n",
        "    for k in range(2, K + 1):\n",
        "        if len(prev) < 2:\n",
        "            return []\n",
        "\n",
        "        # k-uplets uniques triés\n",
        "        # combos = _unique_sorted_rows(np.ascontiguousarray(prev, dtype=np.int64), sort_rows=False)\n",
        "        combos_s = np.ascontiguousarray(prev, dtype=np.int64)\n",
        "        # Barycentres/poids pour tous les combos (batch Cython recommandé)\n",
        "        Q_s = np.empty((combos_s.shape[0], d), dtype=np.float64)\n",
        "        w_s = np.empty((combos_s.shape[0],), dtype=np.float64)\n",
        "        bary_weight_batch(M, s2_all, combos_s, Q_s, w_s)\n",
        "        if verbeux :\n",
        "            print(\"Nouveaux barycentres pondérés calculés.\")\n",
        "\n",
        "        # # Sparsification (sans indices → on les reconstruit)\n",
        "        # if epsilon > 0.0 and combos.shape[0] > 2:\n",
        "        #     Q_s, idx = _sparsify_with_indices(Q, epsilon)\n",
        "        #     if idx.size >= 2:\n",
        "        #         combos_s = combos[idx]\n",
        "        #         w_s = w[idx]\n",
        "        #     else:\n",
        "        #         combos_s, Q_s, w_s = combos, Q, w\n",
        "        # else:\n",
        "        #     combos_s, Q_s, w_s = combos, Q, w\n",
        "\n",
        "        if verbeux :\n",
        "            print(f\"Q_s.shape[0] = {Q_s.shape[0]}\")\n",
        "\n",
        "\n",
        "        # Delaunay pondérée globale sur l’échantillon\n",
        "        e_global = _edges_from_weighted_delaunay(Q_s, w_s, precision=precision)\n",
        "        if verbeux :\n",
        "            print(\"e_global : \", len(e_global))\n",
        "        if len(e_global) == 0:\n",
        "            return []\n",
        "\n",
        "        # Unions k→k+1 (adjacences d'ordre k)\n",
        "        next_candidates = []\n",
        "        out_u = np.empty(k + 1, dtype=np.int64)\n",
        "        for i, j in e_global:\n",
        "            A = combos_s[i]\n",
        "            B = combos_s[j]\n",
        "            if union_if_adjacent_int(A, B, out_u):\n",
        "                next_candidates.append(out_u.copy())\n",
        "\n",
        "        if not next_candidates:\n",
        "            return []\n",
        "\n",
        "        if verbeux :\n",
        "            print(\"next_candidates : \", len(next_candidates))\n",
        "        prev = _unique_sorted_rows(np.asarray(next_candidates, dtype=np.int64), sort_rows=False)\n",
        "        if verbeux :\n",
        "            print(\"prev : \", prev.shape)\n",
        "        prev = np.ascontiguousarray(prev, dtype=np.int64)\n",
        "        if verbeux:\n",
        "            print(f\"orderk_delaunay k = {k}\")\n",
        "\n",
        "    return prev.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hd1fT9o4RUv2",
      "metadata": {
        "id": "Hd1fT9o4RUv2"
      },
      "outputs": [],
      "source": [
        "# ==== Optimized v7: bucketing edges per component, vectorized chains, selectable distinct_mode ====\n",
        "\n",
        "def _kruskal_mst_from_edges(n_nodes, rows, cols, weights, UF):\n",
        "    rows = np.asarray(rows, dtype=np.int64)\n",
        "    cols = np.asarray(cols, dtype=np.int64)\n",
        "    weights = np.asarray(weights, dtype=float)\n",
        "    if rows.size == 0:\n",
        "        return []\n",
        "    order = np.argsort(weights, kind=\"stable\")\n",
        "    rows = rows[order]; cols = cols[order]; weights = weights[order]\n",
        "    mst_u = np.empty(max(0, n_nodes - 1), dtype=np.int64)\n",
        "    mst_v = np.empty_like(mst_u)\n",
        "    mst_w = np.empty_like(mst_u, dtype=float)\n",
        "    taken = 0\n",
        "    for i in range(rows.size):\n",
        "        a = int(rows[i]); b = int(cols[i])\n",
        "        ra = UF.find(a); rb = UF.find(b)\n",
        "        if ra == rb:\n",
        "            continue\n",
        "        mst_u[taken] = a; mst_v[taken] = b; mst_w[taken] = float(weights[i])\n",
        "        UF.union(ra, rb)\n",
        "        taken += 1\n",
        "        if taken == mst_u.size:\n",
        "            break\n",
        "    return list(zip(mst_u[:taken].tolist(), mst_v[:taken].tolist(), mst_w[:taken].tolist()))\n",
        "\n",
        "def build_Z_mst_occurrences_gc(face_vertices,\n",
        "                               mst_faces_sorted,   # [(u,v,w)] ou (u_arr, v_arr, w_arr)\n",
        "                               min_cluster_size,\n",
        "                               verbose=False,\n",
        "                               distinct_mode=\"owner\",\n",
        "                               DBSCAN_threshold=None):\n",
        "\n",
        "    def _fmt_bytes(n):\n",
        "        n = float(n)\n",
        "        if n < 1024: return f\"{n:.0f} B\"\n",
        "        n /= 1024\n",
        "        if n < 1024: return f\"{n:.1f} KiB\"\n",
        "        n /= 1024\n",
        "        if n < 1024: return f\"{n:.1f} MiB\"\n",
        "        n /= 1024\n",
        "        return f\"{n:.2f} GiB\"\n",
        "\n",
        "    def _rss():\n",
        "        try:\n",
        "            import psutil\n",
        "            return psutil.Process().memory_info().rss\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # Faces compactes\n",
        "    F = np.asarray(face_vertices, dtype=np.int64, order=\"C\")\n",
        "    m = int(F.shape[0])\n",
        "    K = int(F.shape[1])\n",
        "    if m == 0:\n",
        "        return np.zeros((0,4), np.float64), np.zeros(0, np.int64), F\n",
        "\n",
        "    if verbose:\n",
        "        r = _rss()\n",
        "        print(f\"[F-GC:0] faces={m}, K={K}, faces_bytes={_fmt_bytes(F.nbytes)}, RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    # Arêtes du MST des faces -> arrays + tri par poids\n",
        "    if isinstance(mst_faces_sorted, tuple) and len(mst_faces_sorted)==3:\n",
        "        u_arr, v_arr, w_arr = mst_faces_sorted\n",
        "        u_arr = np.asarray(u_arr, np.int64)\n",
        "        v_arr = np.asarray(v_arr, np.int64)\n",
        "        w_arr = np.asarray(w_arr, np.float64)\n",
        "    else:\n",
        "        u_arr = np.fromiter((u for u,_,_ in mst_faces_sorted), count=len(mst_faces_sorted), dtype=np.int64)\n",
        "        v_arr = np.fromiter((v for _,v,_ in mst_faces_sorted), count=len(mst_faces_sorted), dtype=np.int64)\n",
        "        w_arr = np.fromiter((w for _,_,w in mst_faces_sorted), count=len(mst_faces_sorted), dtype=np.float64)\n",
        "\n",
        "    if u_arr.size == 0:\n",
        "        return np.zeros((0,4), np.float64), np.zeros(m, np.int64), F\n",
        "\n",
        "    order = np.argsort(w_arr, kind=\"stable\")\n",
        "    u_arr = u_arr[order]; v_arr = v_arr[order]; w_arr = w_arr[order]\n",
        "\n",
        "    if verbose:\n",
        "        r = _rss()\n",
        "        print(f\"[F-GC:1] MST-face edges={u_arr.size}, edges_bytes≈{_fmt_bytes(u_arr.nbytes+v_arr.nbytes+w_arr.nbytes)}, RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    # Structures pour Kruskal au niveau des faces\n",
        "    next_face = np.full(m, -1, dtype=np.int64)\n",
        "    head = np.arange(m, dtype=np.int64)\n",
        "    tail = np.arange(m, dtype=np.int64)\n",
        "    comp_sz = np.ones(m, dtype=np.int64)\n",
        "\n",
        "    UF = UnionFind(m)\n",
        "    cid = np.arange(m, dtype=np.int64)\n",
        "    nxt = int(m)\n",
        "\n",
        "    # Owner des sommets (global vertex ids) pour compter \"distincts\"\n",
        "    n_points = int(F.max()) + 1\n",
        "    owner_root = np.full(n_points, -1, dtype=np.int64)\n",
        "    distinct_count = np.zeros(m, dtype=np.int64)\n",
        "\n",
        "    # Pré-distribution: chaque sommet est d’abord \"owned\" par la première face qui le voit\n",
        "    # et on crédite cette face d’1 distinct. O(m*K) mais c’est linéaire et cache-friendly.\n",
        "    for f in range(m):\n",
        "        row = F[f]\n",
        "        for j in range(K):\n",
        "            v = int(row[j])\n",
        "            if owner_root[v] == -1:\n",
        "                owner_root[v] = f\n",
        "                distinct_count[f] += 1\n",
        "\n",
        "    # Hiérarchie au niveau des faces (Z with mixed dtypes)\n",
        "    Z = np.empty((max(0, m-1),4), dtype=np.float64)#[('cluster_id_big', np.intp),('cluster_id_small', np.intp),('weight', np.float64),('distinct_count', np.intp)])\n",
        "    taken = 0\n",
        "\n",
        "    if verbose:\n",
        "        alloc = (next_face.nbytes + head.nbytes + tail.nbytes + comp_sz.nbytes +\n",
        "                 cid.nbytes + owner_root.nbytes + distinct_count.nbytes + Z.nbytes)\n",
        "        r = _rss()\n",
        "        print(f\"[F-GC:2] alloc aux≈{_fmt_bytes(alloc)}, RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    # Kruskal streaming sur les arêtes du MST des faces\n",
        "    for a, b, w in zip(u_arr, v_arr, w_arr):\n",
        "        ra = UF.find(int(a)); rb = UF.find(int(b))\n",
        "        if ra == rb:\n",
        "            continue\n",
        "        sa = int(comp_sz[ra]); sb = int(comp_sz[rb])\n",
        "        if sa < sb: small, big = ra, rb\n",
        "        else:       small, big = rb, ra\n",
        "\n",
        "        root_big = UF.find(big)\n",
        "\n",
        "        # Compte exact des distincts en scannant la petite composante (faces -> sommets)\n",
        "        uniq_add = 0\n",
        "        i = int(head[small])\n",
        "        while i != -1:\n",
        "            row = F[i]\n",
        "            for j in range(K):\n",
        "                v = int(row[j])\n",
        "                pr = owner_root[v]\n",
        "                # Si ce sommet n’appartient pas déjà à la composante 'big' (post-trouvée), on le bascule et on incrémente\n",
        "                if pr == -1 or UF.find(int(pr)) != root_big:\n",
        "                    owner_root[v] = root_big\n",
        "                    uniq_add += 1\n",
        "            i = int(next_face[i])\n",
        "\n",
        "        new_count = int(distinct_count[root_big]) + int(uniq_add)\n",
        "\n",
        "        # Écrit une ligne de Z: [id_big, id_small, poids, distincts]\n",
        "        Z[taken,0] = float(cid[big])\n",
        "        Z[taken,1] = float(cid[small])\n",
        "        Z[taken,2] = float(w)\n",
        "        Z[taken,3] = float(new_count)\n",
        "        taken += 1\n",
        "\n",
        "        # Concatène les listes de faces small -> big\n",
        "        if head[small] != -1:\n",
        "            if head[big] == -1:\n",
        "                head[big] = head[small]; tail[big] = tail[small]\n",
        "            else:\n",
        "                next_face[int(tail[big])] = int(head[small])\n",
        "                tail[big] = int(tail[small])\n",
        "            comp_sz[big] = comp_sz[big] + comp_sz[small]\n",
        "            head[small] = -1; tail[small] = -1; comp_sz[small] = 0\n",
        "\n",
        "        # Union et propagation des méta-infos au nouveau root\n",
        "        UF.union(big, small)\n",
        "        root = UF.find(big)\n",
        "        head[root] = head[big]; tail[root] = tail[big]; comp_sz[root] = comp_sz[big]\n",
        "        distinct_count[root] = new_count\n",
        "        cid[root] = nxt; nxt += 1\n",
        "\n",
        "        if verbose and (taken % 5_000_000 == 0):\n",
        "            r = _rss()\n",
        "            print(f\"[F-GC:merge] merges={taken}/{m-1}, RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "        if taken == m - 1:\n",
        "            break\n",
        "\n",
        "    Z = Z if taken == Z.shape[0] else Z[:taken].copy()\n",
        "\n",
        "    if verbose:\n",
        "        r = _rss()\n",
        "        print(f\"[F-GC:done] Z(shape={Z.shape}, dtype={Z.dtype}), RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    Z_pruned, surv_idx = prune_linkage_by_inclusion(Z_full=Z, K=K, verbose=verbose)\n",
        "    if verbose:\n",
        "        r = _rss()\n",
        "        print(f\"[F-GC:Tree2Links] Z_pruned.shape[0]={Z_pruned.shape[0]} RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    if Z_pruned.shape[0] <= 1 or Z_pruned[-1,3] <= min_cluster_size :\n",
        "        return Z_pruned.shape, np.full(m, -2, dtype=np.int64), np.zeros(m, np.float64), F\n",
        "\n",
        "    # Clustering sur les FACES (labels par face)\n",
        "    labels_faces, probabilities_faces, *_ = tree_to_labels(\n",
        "        single_linkage_tree=Z_pruned,\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        DBSCAN_threshold=DBSCAN_threshold\n",
        "    )\n",
        "    if verbose:\n",
        "        r = _rss()\n",
        "        print(f\"[F-GC: Tree2Labels]  RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    # On renvoie: Z, labels PAR FACE, et la table face_vertices compacte\n",
        "    ret_labels = np.full(m, -2, dtype=np.int64)\n",
        "    ret_prob = np.zeros(m, dtype=np.float64)\n",
        "    if verbose :\n",
        "        print(len(surv_idx), len(ret_labels),len(ret_labels[surv_idx]),len(labels_faces))\n",
        "    ret_labels[surv_idx] = labels_faces\n",
        "    ret_prob[surv_idx] = probabilities_faces\n",
        "    return Z, ret_labels, ret_prob, F\n",
        "\n",
        "\n",
        "\n",
        "# def prune_linkage_by_inclusion(Z_full: np.ndarray, K: int, verbose: bool=False):\n",
        "#     \"\"\"\n",
        "#     Émonde Z_full par inclusion et reconstruit un linkage SciPy VALIDE uniquement\n",
        "#     sur les feuilles survivantes. La colonne 3 (taille d'union) est reprise EXACTEMENT\n",
        "#     depuis Z_full pour chaque fusion conservée.\n",
        "\n",
        "#     Hypothèses SciPy:\n",
        "#       - m = Z_full.shape[0] + 1 feuilles\n",
        "#       - parents en ordre: parent_i = m + i, et Z[i,0], Z[i,1] < m + i\n",
        "#       - Z[i] = [childL, childR, weight, union_size] avec union_size = |A ∪ B| (“distincts”)\n",
        "#       - chaque feuille vaut implicitement K (utile seulement pour tester l’inclusion au premier niveau)\n",
        "#     \"\"\"\n",
        "#     Z = np.asarray(Z_full, dtype=np.float64, order=\"C\")\n",
        "#     t = int(Z.shape[0])\n",
        "#     m = t + 1\n",
        "#     if m <= 1:\n",
        "#         return np.zeros((0,4), np.float64), np.arange(m, dtype=np.int64)\n",
        "\n",
        "#     left   = Z[:, 0].astype(np.int64,   copy=False)\n",
        "#     right  = Z[:, 1].astype(np.int64,   copy=False)\n",
        "#     weight = Z[:, 2].astype(np.float64, copy=False)\n",
        "#     usize  = Z[:, 3].astype(np.int64,   copy=False)\n",
        "\n",
        "#     n_nodes = m + t\n",
        "\n",
        "#     # Tailles de nœuds pour tester l'inclusion: feuille=K, interne=usize\n",
        "#     size_node = np.zeros(n_nodes, dtype=np.int64)\n",
        "#     size_node[:m]   = K\n",
        "#     size_node[m:]   = usize\n",
        "\n",
        "#     # 1) Élagage par inclusion: DFS sur le \"loser\" (aucune liste partagée)\n",
        "#     alive_leaf = np.ones(m, dtype=bool)\n",
        "#     dead_node  = np.zeros(n_nodes, dtype=bool)\n",
        "\n",
        "#     def kill_subtree(root: int):\n",
        "#         stack = [int(root)]\n",
        "#         while stack:\n",
        "#             x = stack.pop()\n",
        "#             if dead_node[x]:\n",
        "#                 continue\n",
        "#             dead_node[x] = True\n",
        "#             if x < m:\n",
        "#                 alive_leaf[x] = False\n",
        "#             else:\n",
        "#                 i = x - m\n",
        "#                 stack.append(int(left[i]))\n",
        "#                 stack.append(int(right[i]))\n",
        "\n",
        "#     for i in range(t):\n",
        "#         a = int(left[i]); b = int(right[i]); p = m + i\n",
        "#         sa = int(size_node[a]); sb = int(size_node[b]); su = int(size_node[p])\n",
        "#         # inclusion ssi union == max(sa, sb) → on supprime TOUT le sous-arbre perdant\n",
        "#         if su == (sa if sa >= sb else sb):\n",
        "#             loser = b if sa >= sb else a\n",
        "#             kill_subtree(loser)\n",
        "\n",
        "#     # 2) Marque \"alive\" pour tous les nœuds (pas que les feuilles)\n",
        "#     alive_node = np.zeros(n_nodes, dtype=bool)\n",
        "#     alive_node[:m] = alive_leaf\n",
        "#     for i in range(t-1, -1, -1):\n",
        "#         a = int(left[i]); b = int(right[i]); p = m + i\n",
        "#         alive_node[p] = alive_node[a] | alive_node[b]\n",
        "\n",
        "#     # 3) Réindexation et reconstruction en une passe\n",
        "#     #    new_id[x] = id dans l’arbre PRUNÉ, sinon -1\n",
        "#     new_id = np.full(n_nodes, -1, dtype=np.int64)\n",
        "#     # feuilles survivantes → 0..L-1\n",
        "#     surv_idx = np.nonzero(alive_leaf)[0].astype(np.int64)\n",
        "#     L = int(surv_idx.size)\n",
        "#     if L <= 1:\n",
        "#         return np.zeros((0,4), np.float64), surv_idx\n",
        "#     new_id[surv_idx] = np.arange(L, dtype=np.int64)\n",
        "\n",
        "#     # sorties\n",
        "#     Z_pruned = np.empty((L-1, 4), dtype=np.float64)\n",
        "#     created = 0\n",
        "\n",
        "#     for i in range(t):\n",
        "#         a = int(left[i]); b = int(right[i]); p = m + i\n",
        "#         na = int(new_id[a]); nb = int(new_id[b])\n",
        "\n",
        "#         if na == -1 and nb == -1:\n",
        "#             # aucun survivant sous ce parent: tout est mort → parent s’effondre\n",
        "#             new_id[p] = -1\n",
        "#             continue\n",
        "\n",
        "#         if na == -1 or nb == -1:\n",
        "#             # un seul côté vivant: on \"raccroche\" en se rétractant sur le côté vivant\n",
        "#             new_id[p] = nb if na == -1 else na\n",
        "#             continue\n",
        "\n",
        "#         # deux côtés vivants: on GARDE la fusion\n",
        "#         ida, idb = (na, nb) if na < nb else (nb, na)  # ordre esthétique\n",
        "#         parent_new = L + created\n",
        "\n",
        "#         Z_pruned[created, 0] = float(ida)\n",
        "#         Z_pruned[created, 1] = float(idb)\n",
        "#         Z_pruned[created, 2] = float(weight[i])\n",
        "#         Z_pruned[created, 3] = float(usize[i])   # on garde exactement la taille d’union d’origine\n",
        "\n",
        "#         new_id[p] = parent_new\n",
        "#         created += 1\n",
        "#         if created == L - 1:\n",
        "#             break\n",
        "\n",
        "#     if created != L - 1:\n",
        "#         # Si ça arrive, c’est que Z_full viole la convention SciPy (ou a été corrompu).\n",
        "#         raise RuntimeError(f\"[PRUNE] Quotient incomplet: L={L}, edges={created}, attendu {L-1}\")\n",
        "\n",
        "#     if verbose:\n",
        "#         print(f\"[PRUNE] feuilles: {m} → survivantes: {L} | Z_pruned: {Z_pruned.shape}\")\n",
        "\n",
        "#     return Z_pruned, surv_idx\n",
        "\n",
        "# # Helper: construit l’ordre DFS des feuilles et les intervalles [first,last] de feuilles pour chaque nœud\n",
        "# def build_leaf_dfs_intervals(left: np.ndarray, right: np.ndarray):\n",
        "#     \"\"\"\n",
        "#     Retourne:\n",
        "#       pos        : (m,) int64, mapping leaf_id -> position DFS [0..m-1]\n",
        "#       first,last : (m+t,) int64, bornes d’intervalle de feuilles pour chaque nœud (0..m+t-1)\n",
        "#       leaf_order : (m,) int64, inverse de pos (leaf_id_by_pos)\n",
        "#     Hypothèse SciPy: parent_i = m+i, enfants < parent.\n",
        "#     \"\"\"\n",
        "#     t = int(left.size)\n",
        "#     m = t + 1\n",
        "#     n_nodes = m + t\n",
        "\n",
        "#     first = np.full(n_nodes, -1, dtype=np.int64)\n",
        "#     last  = np.full(n_nodes, -1, dtype=np.int64)\n",
        "#     leaf_order = np.empty(m, dtype=np.int64)\n",
        "\n",
        "#     root = m + t - 1\n",
        "#     stack = [(int(root), 0)]\n",
        "#     k = 0\n",
        "#     while stack:\n",
        "#         x, state = stack.pop()\n",
        "#         if x < m:\n",
        "#             first[x] = last[x] = k\n",
        "#             leaf_order[k] = x\n",
        "#             k += 1\n",
        "#         else:\n",
        "#             i = x - m\n",
        "#             if state == 0:\n",
        "#                 stack.append((x, 1))\n",
        "#                 stack.append((int(right[i]), 0))\n",
        "#                 stack.append((int(left[i]), 0))\n",
        "#             else:\n",
        "#                 a = int(left[i]); b = int(right[i])\n",
        "#                 fa, fb = int(first[a]), int(first[b])\n",
        "#                 la, lb = int(last[a]),  int(last[b])\n",
        "#                 if fa == -1 or fb == -1:\n",
        "#                     raise RuntimeError(\"Arbre invalide: intervalle enfant manquant.\")\n",
        "#                 first[x] = fa if fa <= fb else fb\n",
        "#                 last[x]  = la if la >= lb else lb\n",
        "\n",
        "#     if k != m:\n",
        "#         raise RuntimeError(\"DFS feuilles incomplet.\")\n",
        "#     pos = np.empty(m, dtype=np.int64)\n",
        "#     pos[leaf_order] = np.arange(m, dtype=np.int64)\n",
        "#     return pos, first, last, leaf_order\n",
        "\n",
        "\n",
        "def prune_linkage_by_inclusion(Z_full: np.ndarray, K: int, verbose: bool=False):\n",
        "    \"\"\"\n",
        "    Option 2 robuste (compression par inclusion) compatible avec ta UnionFind Cython (find/union).\n",
        "\n",
        "    Étapes:\n",
        "      1) Calcule les intervalles de feuilles [first,last] pour chaque nœud via DFS.\n",
        "      2) Marque les fusions inclusives (su == max(sa,sb)) et contracte TOUTE la branche perdante\n",
        "         en unionnant l’intervalle de feuilles perdant dans la classe de l’intervalle gagnant.\n",
        "      3) Construit un graphe d’arêtes entre classes pour chaque fusion non inclusive.\n",
        "      4) Kruskal sur les classes → Z_pruned de taille (L'−1,4), avec la 3e colonne copiée depuis Z_full.\n",
        "      5) Renvoie aussi surv_idx de **taille L'**: pour chaque classe, un id de feuille d’origine représentante.\n",
        "\n",
        "    Retour:\n",
        "      Z_pruned : np.ndarray, shape (L'−1, 4), dtype float64\n",
        "      surv_idx : np.ndarray, shape (L',), dtype int64   (un id de feuille d’origine par classe)\n",
        "    \"\"\"\n",
        "    Z = np.asarray(Z_full, dtype=np.float64, order=\"C\")\n",
        "    t = int(Z.shape[0])\n",
        "    m = t + 1\n",
        "    if m <= 1:\n",
        "        # 0 fusion -> 1 classe, mais on n’a pas de feuille à choisir; on met 0 par convention si m==1\n",
        "        return np.zeros((0, 4), np.float64), np.array([0], dtype=np.int64)\n",
        "\n",
        "    left   = Z[:, 0].astype(np.int64,   copy=False)\n",
        "    right  = Z[:, 1].astype(np.int64,   copy=False)\n",
        "    weight = Z[:, 2].astype(np.float64, copy=False)\n",
        "    usize  = np.rint(Z[:, 3]).astype(np.int64, copy=False)\n",
        "\n",
        "    # Garde SciPy\n",
        "    for i in range(t):\n",
        "        p = m + i\n",
        "        a = int(left[i]); b = int(right[i])\n",
        "        if not (0 <= a < p and 0 <= b < p):\n",
        "            raise ValueError(f\"Convention SciPy violée à i={i}: enfants {a},{b} >= parent {p}\")\n",
        "\n",
        "    n_nodes = m + t\n",
        "\n",
        "    # Tailles au moment des fusions: feuilles=K, internes=usize\n",
        "    size_node = np.zeros(n_nodes, dtype=np.int64)\n",
        "    size_node[:m] = int(K)\n",
        "    size_node[m:] = usize\n",
        "\n",
        "    # Inclusives (ordre indépendant): su == max(sa, sb)\n",
        "    inc = np.zeros(t, dtype=bool)\n",
        "    for i in range(t):\n",
        "        a = int(left[i]); b = int(right[i]); p = m + i\n",
        "        sa, sb, su = int(size_node[a]), int(size_node[b]), int(size_node[p])\n",
        "        inc[i] = (su == (sa if sa >= sb else sb))\n",
        "\n",
        "    # Intervalles de feuilles\n",
        "    pos, first, last, leaf_order = build_leaf_dfs_intervals(left, right)  # pos: leaf_id -> dfs_idx\n",
        "\n",
        "    # Union-Find sur les feuilles (indexées par position DFS) + \"next pointer\" pour unions d’intervalles\n",
        "    UF = UnionFind(m)           # ← ta version Cython\n",
        "    nxt = np.arange(m + 1, dtype=np.int64)  # pointeurs de saut\n",
        "\n",
        "    def _next(i: int) -> int:\n",
        "        j = i\n",
        "        while nxt[j] != j:\n",
        "            nxt[j] = nxt[nxt[j]]\n",
        "            j = nxt[j]\n",
        "        return j\n",
        "\n",
        "    def union_interval(L: int, R: int, to_pos: int):\n",
        "        root_to = UF.find(to_pos)\n",
        "        i = _next(L)\n",
        "        while i <= R:\n",
        "            UF.union(i, root_to)\n",
        "            nxt[i] = _next(i + 1)\n",
        "            i = nxt[i]\n",
        "\n",
        "    # Contraction: pour chaque inclusion, on fusionne TOUT l’intervalle perdant dans la classe du gagnant\n",
        "    # L’ordre n’a pas d’importance ici; faire croissant améliore juste la localité mémoire.\n",
        "    for i in range(t):\n",
        "        if not inc[i]:\n",
        "            continue\n",
        "        a = int(left[i]); b = int(right[i])\n",
        "        sa, sb = int(size_node[a]), int(size_node[b])\n",
        "        winner = a if sa >= sb else b\n",
        "        loser  = b if sa >= sb else a\n",
        "        Lw, Rw = int(first[winner]), int(last[winner])\n",
        "        Ll, Rl = int(first[loser]),  int(last[loser])\n",
        "        rep_pos = UF.find(Lw)\n",
        "        union_interval(Ll, Rl, rep_pos)\n",
        "\n",
        "    # Classes sur feuilles (en positions DFS)\n",
        "    rep_pos = np.array([UF.find(i) for i in range(m)], dtype=np.int64)\n",
        "    uniq_rep, inverse = np.unique(rep_pos, return_inverse=True)  # uniq_rep: reps de classes, triés\n",
        "    Lp = int(uniq_rep.size)\n",
        "    cls_of_pos = inverse  # mapping position DFS -> classe [0..Lp-1]\n",
        "\n",
        "    # Choix d’un représentant feuille pour CHAQUE classe (ici: la feuille d’origine de plus petite position DFS)\n",
        "    # surv_idx de taille L' demandé: un id de feuille d’origine par classe\n",
        "    min_pos_per_class = np.full(Lp, m + 1, dtype=np.int64)\n",
        "    for p in range(m):\n",
        "        c = int(cls_of_pos[p])\n",
        "        if p < min_pos_per_class[c]:\n",
        "            min_pos_per_class[c] = p\n",
        "    # convertit position DFS -> leaf_id d’origine via leaf_order\n",
        "    surv_idx = leaf_order[min_pos_per_class]  # shape (Lp,), dtype int64\n",
        "\n",
        "    if Lp <= 1:\n",
        "        return np.zeros((0, 4), np.float64), surv_idx\n",
        "\n",
        "    # Arêtes pour fusions NON inclusives: relie une classe de gauche à une de droite\n",
        "    U = []\n",
        "    V = []\n",
        "    W = []\n",
        "    S = []\n",
        "    for i in range(t):\n",
        "        if inc[i]:\n",
        "            continue\n",
        "        a = int(left[i]); b = int(right[i])\n",
        "        La = int(first[a]); Lb = int(first[b])  # un représentant de chaque côté suffit\n",
        "        ca = int(cls_of_pos[UF.find(La)])\n",
        "        cb = int(cls_of_pos[UF.find(Lb)])\n",
        "        if ca == cb:\n",
        "            continue\n",
        "        if ca > cb:\n",
        "            ca, cb = cb, ca\n",
        "        U.append(ca); V.append(cb); W.append(float(weight[i])); S.append(float(usize[i]))\n",
        "\n",
        "    if not U:\n",
        "        raise RuntimeError(f\"[PRUNE/COMPRESS] Aucune arête non inclusive entre {Lp} classes.\")\n",
        "\n",
        "    U = np.asarray(U, dtype=np.int64); V = np.asarray(V, dtype=np.int64)\n",
        "    W = np.asarray(W, dtype=np.float64); S = np.asarray(S, dtype=np.float64)\n",
        "\n",
        "    # Kruskal sur les classes\n",
        "    UFc = UnionFind(Lp)  # ← ta Cython UF\n",
        "    comp_id = np.arange(Lp, dtype=np.int64)\n",
        "    Z_pruned = np.empty((Lp - 1, 4), dtype=np.float64)\n",
        "    created = 0\n",
        "    for i in range(U.size):\n",
        "        ru = UFc.find(int(U[i])); rv = UFc.find(int(V[i]))\n",
        "        if ru == rv:\n",
        "            continue\n",
        "        ida, idb = int(comp_id[ru]), int(comp_id[rv])\n",
        "        if ida > idb:\n",
        "            ida, idb = idb, ida\n",
        "        Z_pruned[created, 0] = float(ida)\n",
        "        Z_pruned[created, 1] = float(idb)\n",
        "        Z_pruned[created, 2] = float(W[i])\n",
        "        Z_pruned[created, 3] = float(S[i])\n",
        "        UFc.union(ru, rv)\n",
        "        root = UFc.find(ru)\n",
        "        comp_id[root] = Lp + created\n",
        "        created += 1\n",
        "        if created == Lp - 1:\n",
        "            break\n",
        "\n",
        "    if created != Lp - 1:\n",
        "        raise RuntimeError(f\"[PRUNE/COMPRESS] Quotient incomplet: classes={Lp}, edges={created}, attendu {Lp-1}.\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[PRUNE/COMPRESS] feuilles={m}, classes={Lp} | Z_pruned: {Z_pruned.shape}\")\n",
        "\n",
        "    return Z_pruned, surv_idx\n",
        "\n",
        "\n",
        "def build_Z_mst_occurrences_components(face_vertices,\n",
        "                                       mst_faces_sorted,    # [(u,v,w)]\n",
        "                                       min_cluster_size,\n",
        "                                       verbose=False,\n",
        "                                       distinct_mode=\"owner\",\n",
        "                                       DBSCAN_threshold=None):\n",
        "    def _fmt_bytes(n):\n",
        "        n = float(n)\n",
        "        if n < 1024: return f\"{n:.0f} B\"\n",
        "        n /= 1024\n",
        "        if n < 1024: return f\"{n:.1f} KiB\"\n",
        "        n /= 1024\n",
        "        if n < 1024: return f\"{n:.1f} MiB\"\n",
        "        n /= 1024\n",
        "        return f\"{n:.2f} GiB\"\n",
        "\n",
        "    def _rss():\n",
        "        try:\n",
        "            import psutil\n",
        "            return psutil.Process().memory_info().rss\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    face_vertices = np.asarray(face_vertices, dtype=np.int64, order=\"C\")\n",
        "    n_faces = int(face_vertices.shape[0])\n",
        "    if n_faces == 0:\n",
        "        return np.zeros(0, dtype=np.int64)\n",
        "\n",
        "    # Composantes de FACES via UF sur MST des faces\n",
        "    UF_face = UnionFind(n_faces)\n",
        "    for u, v, _ in mst_faces_sorted:\n",
        "        UF_face.union(u, v)\n",
        "    comp_labels = np.fromiter((UF_face.find(i) for i in range(n_faces)), count=n_faces, dtype=np.int64)\n",
        "\n",
        "    # Faces triées par composante\n",
        "    order_faces = np.argsort(comp_labels, kind='mergesort')\n",
        "    labels_sorted = comp_labels[order_faces]\n",
        "    diff_faces = labels_sorted[1:] != labels_sorted[:-1]\n",
        "    starts = np.r_[0, 1 + np.flatnonzero(diff_faces)]\n",
        "    ends   = np.r_[starts[1:], labels_sorted.size]\n",
        "    uniq = labels_sorted[starts]\n",
        "    faces_ordered = np.arange(n_faces, dtype=np.int64)[order_faces]\n",
        "\n",
        "    # Préparation du mapping points -> labels\n",
        "    n_points_total = int(face_vertices.max()) + 1\n",
        "    labels_points_unique  = np.full(n_points_total, -1, dtype=np.int64)\n",
        "    labels_points_multiple = [[] for _ in range(n_points_total)]\n",
        "    first    = np.full(n_points_total, -2, dtype=np.int64)\n",
        "    conflict = np.zeros(n_points_total, dtype=bool)\n",
        "    next_cluster_id = 0\n",
        "\n",
        "    if verbose:\n",
        "        r = _rss()\n",
        "        if r is not None:\n",
        "            print(f\"[COMP-F:0] components={uniq.size}, faces={n_faces}, points={n_points_total}, RSS={_fmt_bytes(r)}\")\n",
        "\n",
        "    # Pour chaque composante de FACES, on calcule le Z (faces) et labels de faces, puis on propage aux points\n",
        "    if len(mst_faces_sorted):\n",
        "        u_arr = np.asarray([uv[0] for uv in mst_faces_sorted], dtype=np.int64)\n",
        "        v_arr = np.asarray([uv[1] for uv in mst_faces_sorted], dtype=np.int64)\n",
        "        w_arr = np.asarray([uv[2] for uv in mst_faces_sorted], dtype=np.float64)\n",
        "        comp_u = comp_labels[u_arr]\n",
        "        order_edges = np.argsort(comp_u, kind='mergesort')\n",
        "        comp_u_sorted = comp_u[order_edges]\n",
        "        diff_edges = comp_u_sorted[1:] != comp_u_sorted[:-1]\n",
        "        starts_e = np.r_[0, 1 + np.flatnonzero(diff_edges)]\n",
        "        ends_e   = np.r_[starts_e[1:], comp_u_sorted.size]\n",
        "        uniq_e = comp_u_sorted[starts_e]\n",
        "    else:\n",
        "        u_arr = v_arr = np.empty(0, dtype=np.int64); w_arr = np.empty(0, dtype=np.float64)\n",
        "        order_edges = np.empty(0, dtype=np.int64)\n",
        "        starts_e = ends_e = uniq_e = np.empty(0, dtype=np.int64)\n",
        "\n",
        "    for j in range(uniq.size):\n",
        "        f_start, f_end = int(starts[j]), int(ends[j])\n",
        "        faces = faces_ordered[f_start:f_end]\n",
        "        if faces.size == 0:\n",
        "            continue\n",
        "        comp_id = int(uniq[j])\n",
        "\n",
        "        # Sous-ensemble d'arêtes du MST des faces pour cette composante\n",
        "        if order_edges.size:\n",
        "            pos = np.searchsorted(uniq_e, comp_id)\n",
        "            if pos < uniq_e.size and uniq_e[pos] == comp_id:\n",
        "                e_start = int(starts_e[pos]); e_end = int(ends_e[pos])\n",
        "                idx_edges = order_edges[e_start:e_end]\n",
        "            else:\n",
        "                idx_edges = np.empty(0, dtype=np.int64)\n",
        "        else:\n",
        "            idx_edges = np.empty(0, dtype=np.int64)\n",
        "\n",
        "        faces_sorted = np.sort(faces)\n",
        "        # Remap local\n",
        "        u_sel = u_arr[idx_edges]; v_sel = v_arr[idx_edges]; w_sel = w_arr[idx_edges]\n",
        "        new_u = np.searchsorted(faces_sorted, u_sel).astype(np.int64, copy=False)\n",
        "        new_v = np.searchsorted(faces_sorted, v_sel).astype(np.int64, copy=False)\n",
        "\n",
        "        faces_compact = face_vertices[faces_sorted]\n",
        "\n",
        "        if verbose:\n",
        "            r = _rss()\n",
        "            est_Z = max(0, faces_compact.shape[0]-1)*4*8\n",
        "            print(f\"[COMP-F:1] comp {j+1}/{uniq.size} | faces={faces_compact.shape[0]}, edges={new_u.size}, \"\n",
        "                  f\"Z_est≈{_fmt_bytes(est_Z)}, RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "        # GC au niveau des FACES (plus d’occurrences ici)\n",
        "        Z_i, labels_faces_i, probabilities_faces_i,  F_i = build_Z_mst_occurrences_gc(\n",
        "            faces_compact, (new_u, new_v, w_sel),\n",
        "            min_cluster_size=min_cluster_size, verbose=verbose,\n",
        "            distinct_mode=\"owner\",\n",
        "            DBSCAN_threshold=DBSCAN_threshold\n",
        "        )\n",
        "\n",
        "        # Propagation labels de faces -> points, avec gestion des conflits\n",
        "        if len(labels_faces_i) and np.any(labels_faces_i != -1):\n",
        "            max_local = int(labels_faces_i[labels_faces_i != -1].max())\n",
        "            offset = next_cluster_id\n",
        "            for f_loc in range(F_i.shape[0]):\n",
        "                lbl = int(labels_faces_i[f_loc])\n",
        "                if lbl == -2 or lbl == -1 :\n",
        "                    continue\n",
        "                # if lbl != -1 :\n",
        "                lbl_g = lbl + offset\n",
        "                # else :\n",
        "                #     lbl_g = -1\n",
        "                # pour chaque sommet de la face f_loc\n",
        "                row = F_i[f_loc]\n",
        "                for t in range(row.size):\n",
        "                    v = int(row[t])\n",
        "                    labels_points_multiple[v].append((lbl_g, probabilities_faces_i[f_loc]))\n",
        "                    if first[v] == -2:\n",
        "                        first[v] = lbl_g\n",
        "                    elif first[v] != lbl_g:\n",
        "                        conflict[v] = True\n",
        "            next_cluster_id = offset + max_local + 1\n",
        "\n",
        "        if verbose and (j % 1 == 0):\n",
        "            valid = (first != -2) & (~conflict)\n",
        "            r = _rss()\n",
        "            print(f\"[COMP-F:2] comp {j+1}/{uniq.size} done | cumul points labellisés={int(valid.sum())} \"\n",
        "                  f\"| conflits={int(conflict.sum())} | RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    # Finalisation des labels Points\n",
        "    mask_ok = (~conflict) & (first != -2)\n",
        "    labels_points_unique[mask_ok] = first[mask_ok]\n",
        "    ret_labels_points_multiple = [[] for _ in range(n_points_total)]\n",
        "    for v, labels in enumerate(labels_points_multiple):\n",
        "        dict_labels = {}\n",
        "        labels_unique = []\n",
        "        for ind,(lbl,prob) in enumerate(labels) :\n",
        "            if lbl not in dict_labels :\n",
        "                dict_labels[lbl] = [ind]\n",
        "            else :\n",
        "                dict_labels[lbl].append(ind)\n",
        "        for lbl,inds in dict_labels.items() :\n",
        "            size_lbl = len(inds)\n",
        "            prob_lbl = 0\n",
        "            for ind in inds :\n",
        "                prob_lbl += labels[ind][1]\n",
        "            prob_lbl /= size_lbl\n",
        "            labels_unique.append((lbl,size_lbl/len(labels),prob_lbl))\n",
        "        if labels_unique == [] :\n",
        "            labels_unique = [(-1,1,1)]\n",
        "        ret_labels_points_multiple[v] = labels_unique\n",
        "\n",
        "    if verbose:\n",
        "        uvals = np.unique(labels_points_unique)\n",
        "        r = _rss()\n",
        "        print('Clusters finaux :', uvals[uvals != -1].size, '| bruit :', int(np.sum(labels_points_unique == -1)),\n",
        "              f\"| RSS={_fmt_bytes(r) if r else 'n/a'}\")\n",
        "\n",
        "    return labels_points_unique, ret_labels_points_multiple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GnyZmMCljTCF",
      "metadata": {
        "id": "GnyZmMCljTCF"
      },
      "outputs": [],
      "source": [
        "# Clone du repo (pas de release/pip officiel)\n",
        "# !git clone -q https://github.com/geoo89/orderkdelaunay.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2EJpmaCcjXvn",
      "metadata": {
        "id": "2EJpmaCcjXvn"
      },
      "outputs": [],
      "source": [
        "# import sys, os, pathlib\n",
        "\n",
        "# repo = pathlib.Path(\"/content/orderkdelaunay\")\n",
        "# pkg_path = repo / \"python\"\n",
        "# assert pkg_path.exists(), \"Le dossier 'python' n'a pas été trouvé dans le repo.\"\n",
        "# sys.path.insert(0, str(pkg_path))\n",
        "\n",
        "# # Import de la classe Python (décrite dans le README du dépôt)\n",
        "# from orderk_delaunay import OrderKDelaunay\n",
        "\n",
        "# print(\"Import OK:\", OrderKDelaunay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KvVjfraFjoBX",
      "metadata": {
        "id": "KvVjfraFjoBX"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Petit nuage 2D bidon\n",
        "# rng = np.random.default_rng(0)\n",
        "# X = rng.random((20, 2))\n",
        "\n",
        "# # Instancier l’algorithme (API exacte: voir help() ci-dessous)\n",
        "# okdel = OrderKDelaunay(X,2)\n",
        "# print(\"Instance créée.\")\n",
        "\n",
        "# # Découvre l’API dispo dans cette version\n",
        "# print([name for name in dir(okdel) if \"mosaic\" in name.lower() or \"order\" in name.lower()])\n",
        "# help(OrderKDelaunay)  # scrolle si tu as du courage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bzWABpeakVIX",
      "metadata": {
        "id": "bzWABpeakVIX"
      },
      "outputs": [],
      "source": [
        "# points = rng.random((10, 2))\n",
        "# # from plotter import Plotter, Plotter2D, Plotter3D\n",
        "\n",
        "# # the order k up to which to compute the order-k Delaunay diagram\n",
        "# order = 3\n",
        "# # Whether to print the cells of all the complexes\n",
        "# print_output = True\n",
        "# # Whether to draw all the order-k Delaunay mosaics\n",
        "# draw_output = True\n",
        "\n",
        "# # Compute the order-k Delaunay mosaics\n",
        "# orderk_delaunay = OrderKDelaunay(points, order)\n",
        "\n",
        "# # Initialize appropriate plotter for drawing the mosaics.\n",
        "# dimension = len(points[0])\n",
        "# if dimension == 2:\n",
        "#     plotter = Plotter2D(points, orderk_delaunay)\n",
        "# elif dimension == 3:\n",
        "#     plotter = Plotter3D(points, orderk_delaunay)\n",
        "# else:\n",
        "#     # Stub that doesn't draw anything.\n",
        "#     plotter = Plotter(points, orderk_delaunay)\n",
        "\n",
        "# for k in range(1, order+1):\n",
        "#     if draw_output:\n",
        "#         plotter.draw(k)\n",
        "\n",
        "#     if print_output:\n",
        "#         cells = orderk_delaunay.diagrams_cells[k-1]\n",
        "#         # Output all the cells.\n",
        "#         print(\"Order {}. Number of cells: {}\".format(\n",
        "#                 len(cells[0][0]), len(cells)))\n",
        "#         print(len(orderk_delaunay.diagrams_simplices[k-1]), orderk_delaunay.diagrams_vertices[k-1])\n",
        "#         for cell in sorted(cells):\n",
        "#             print(cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338165d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "338165d7",
        "outputId": "23795c4e-0c2d-4932-a053-207a9223aa28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for MiniballCpp (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import math, sys\n",
        "from collections import defaultdict\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.decomposition import PCA\n",
        "from itertools import combinations\n",
        "from operator import itemgetter\n",
        "!pip -q install MiniballCpp\n",
        "import miniball  # oui, le nom est malheureux = cyminiball\n",
        "# import importlib, importlib.metadata as im\n",
        "\n",
        "\n",
        "\n",
        "def _kth_radius(M,k,metric,pre):\n",
        "    if pre:\n",
        "        return np.partition(M,k,axis=1)[:,k]\n",
        "    nn=NearestNeighbors(n_neighbors=k+1,metric=metric).fit(M)\n",
        "    d,_=nn.kneighbors(M)\n",
        "    return d[:,k]\n",
        "\n",
        "def MEB(points_sub):\n",
        "    \"\"\"\n",
        "    Centre, Rayon^2 MEB d'un sous-ensemble de points (k+1, d).\n",
        "    Utilise miniball. Retourne 0. si un seul point.\n",
        "    \"\"\"\n",
        "    if points_sub.shape[0] <= 1:\n",
        "        return points_sub[0], 0.0\n",
        "    if points_sub.shape[0] == 2:\n",
        "        # edge: (||p-q||/2)^2\n",
        "        diff = points_sub[0] - points_sub[1]\n",
        "        return 0.5 * (points_sub[0] + points_sub[1]), float(np.dot(diff, diff))*0.25\n",
        "    # k>=2\n",
        "    mb = miniball.Miniball(points_sub)\n",
        "    center, r2 = mb.center(), mb.squared_radius()\n",
        "    return center, r2\n",
        "\n",
        "\n",
        "\n",
        "# complex = \\in [\"auto\", \"orderk_delaunay\", \"delaunay\", \"rips\"]\n",
        "# precision in [\"safe\", \"exact\"]\n",
        "def _build_graph_KSimplexes(M,K,min_samples,metric,complex_chosen,expZ,precision=\"safe\",verbeux=False) :\n",
        "    if min_samples is None or min_samples <= K :\n",
        "        min_samples = K+1\n",
        "    pre = metric=='precomputed'\n",
        "    Delaunay_possible = not pre and metric=='euclidean' and M.ndim == 2 and M.shape[0] != M.shape[1]\n",
        "    n,d = M.shape[0],M.shape[1]\n",
        "    if complex_chosen.lower() not in [\"orderk_delaunay\", \"delaunay\", \"rips\"] : # \"auto\"\n",
        "        if not Delaunay_possible :\n",
        "            complex_chosen = \"rips\"\n",
        "        else :\n",
        "            if d > 10 and n > 100 :\n",
        "                complex_chosen = \"rips\"\n",
        "            elif d > 10 :\n",
        "                complex_chosen = \"delaunay\"\n",
        "            elif d > 5 and n > 1000 :\n",
        "                complex_chosen = \"rips\"\n",
        "            else :\n",
        "                complex_chosen = \"orderk_delaunay\"\n",
        "    Simplexes = []\n",
        "    if complex_chosen.lower() == \"orderk_delaunay\" :\n",
        "        Simplexes_sans_f = []\n",
        "        if d >= 3 or True :\n",
        "            Simplexes_sans_f = orderk_delaunay3(M, min_samples-1, precision=precision, verbeux=verbeux)\n",
        "            if verbeux :\n",
        "                print(f\"Simplexes sans filtration : {len(Simplexes_sans_f), Simplexes_sans_f[:5]}\")\n",
        "        else :\n",
        "            set_simplexes = set()\n",
        "            orderk_delaunay = OrderKDelaunay(M, min_samples-1,verbeux)\n",
        "            cells = orderk_delaunay.diagrams_cells[min_samples-1-1]\n",
        "            for cell in cells :\n",
        "                for s1,s2 in itertools.combinations(cell,2) :\n",
        "                    vertices_set = set()\n",
        "                    for p in s1 :\n",
        "                        vertices_set.add(int(p))\n",
        "                    for p in s2 :\n",
        "                        vertices_set.add(int(p))\n",
        "                    vertices = list(sorted(vertices_set))\n",
        "                    simplexes = itertools.combinations(vertices, K+1)\n",
        "                    for simplexe in simplexes :\n",
        "                        simplexe = tuple(sorted(simplexe))\n",
        "                        if not simplexe in set_simplexes :\n",
        "                            set_simplexes.add(simplexe)\n",
        "                            simplexe = list(sorted(simplexe))\n",
        "                            Simplexes_sans_f.append(simplexe)\n",
        "        def _sqr_radius_for_simplex(simplex):\n",
        "            idx = np.asarray(simplex, dtype=np.int64)        # indices 1D vers les points de M\n",
        "            P = M[idx]                                      # petite copie (d+1, d) → ok\n",
        "            return miniball.Miniball(P).squared_radius()\n",
        "        # !pip -q install joblib\n",
        "        from joblib import Parallel, delayed\n",
        "        radii_sq = Parallel(n_jobs=N_cpu, prefer=\"processes\")(delayed(_sqr_radius_for_simplex)(s) for s in Simplexes_sans_f)\n",
        "        # radii_sq = Parallel(n_jobs=N_cpu, prefer=\"processes\", batch_size=max(1, len(Simplexes_sans_f)//64), max_nbytes=\"256M\")(delayed(lambda A, s: miniball.Miniball(A[s]).squared_radius())(M, s) for s in Simplexes_sans_f)\n",
        "        if expZ != 2 :\n",
        "            radii_sq = np.asarray(radii_sq)\n",
        "            radii_sq = radii_sq ** (expZ / 2)\n",
        "        Simplexes = [(s,radii_sq[ind]) for ind,s in enumerate(Simplexes_sans_f)]\n",
        "\n",
        "        # ch = max(1, len(Simplexes)//(N_cpu*8))   # ~64 paquets\n",
        "        # with ProcessPoolExecutor(max_workers=N) as ex:\n",
        "        #     radii_sq = list(ex.map(_sqr_radius_for_simplex, Simplexes, chunksize=ch))\n",
        "    else : # \"delaunay\" or \"rips\"\n",
        "        import gudhi\n",
        "        r = _kth_radius(M,min_samples-1,metric,pre)\n",
        "        r2 = r**2\n",
        "        if complex_chosen.lower() == \"rips\" :\n",
        "            r2 = r\n",
        "            expZ *= 2\n",
        "            if precision == \"exact\" :\n",
        "                mx=2*np.quantile(r,0.99)\n",
        "            else : # \"safe\"\n",
        "                mx=(1 + 1/np.sqrt(d))*np.quantile(r,0.99)\n",
        "            if pre or metric!='euclidean':\n",
        "                D=M if pre else pairwise_distances(M,metric=metric)\n",
        "                st=gudhi.RipsComplex(distance_matrix=D,max_edge_length=mx).create_simplex_tree(max_dimension=K)\n",
        "            else: # petite optimisation : on ne construit que jusqu'à la dimension K-1\n",
        "                st=gudhi.RipsComplex(points=M,max_edge_length=mx).create_simplex_tree(max_dimension=K)\n",
        "        else : # complex_chosen.lower() == \"delaunay\" :\n",
        "            st = gudhi.DelaunayCechComplex(points=M).create_simplex_tree()\n",
        "        for s, f in st.get_skeleton(K):\n",
        "            if len(s) != K + 1:\n",
        "                continue\n",
        "            simplexe = list(sorted(s))\n",
        "            max_kth_radius2 = 0\n",
        "            for p in s:\n",
        "                max_kth_radius2 = max(max_kth_radius2, r2[p])\n",
        "            f = max(f, max_kth_radius2)\n",
        "            if expZ != 2:\n",
        "                f = f ** (expZ / 2)\n",
        "            Simplexes.append((simplexe,f))\n",
        "    faces_raw = []\n",
        "    e_u, e_v, e_w = [], [], []\n",
        "    nS = 0\n",
        "    for s, f in Simplexes :\n",
        "        if len(s) <= K :\n",
        "            continue\n",
        "        for KP1_items_choisis in itertools.combinations(range(len(s)), K+1) :\n",
        "            nS += 1\n",
        "            vert = tuple(sorted(KP1_items_choisis))\n",
        "            base = len(faces_raw)\n",
        "            for drop in range(K + 1):\n",
        "                faces_raw.append([s[vert[i]] for i in range(K + 1) if i != drop])\n",
        "            for k in range(K):\n",
        "                e_u.append(base + k)\n",
        "                e_v.append(base + k + 1)\n",
        "                e_w.append(float(f))\n",
        "    return faces_raw, e_u, e_v, e_w, nS\n",
        "\n",
        "\n",
        "def HypergraphPercol(M, K=2, min_cluster_size=None, min_samples=None, metric=\"euclidean\", DBSCAN_threshold=None, label_all_points=False, return_multi_clusters=False, complex_chosen=\"auto\", expZ=2, precision=\"safe\", dim_reducer=False, threshold_variance_dim_reduction=0.999, verbeux=False):\n",
        "    n,d=M.shape[0],M.shape[1]\n",
        "    M = np.ascontiguousarray(M, dtype=np.float64)\n",
        "    if min_cluster_size is None :\n",
        "        min_cluster_size = round(np.sqrt(n))\n",
        "    X = np.copy(M)\n",
        "    pre = metric=='precomputed'\n",
        "    Delaunay_possible = not pre and metric=='euclidean' and M.ndim == 2 and M.shape[0] != M.shape[1]\n",
        "    if min_samples is None or min_samples <= K :\n",
        "        min_samples = K+1\n",
        "    if str(dim_reducer).lower() in ['pca', 'umap'] and Delaunay_possible:\n",
        "        pca = PCA(n_components=threshold_variance_dim_reduction, svd_solver=\"full\", whiten=False)\n",
        "        X2 = pca.fit_transform(M)\n",
        "        r  = pca.n_components_                     # dimension retenue\n",
        "        ratio = pca.explained_variance_ratio_.sum()  # variance conservée (≈ 0.999)\n",
        "        if r < d :\n",
        "            if dim_reducer.lower() == 'pca' :\n",
        "                X = X2\n",
        "                if verbeux :\n",
        "                    print(\"Dimension réduite par Analyse en Composantes Principales (PCA) :\\n d → r : \",d,\"→\",r,\" Variance conservée :\",ratio)\n",
        "            else : # 'umap'\n",
        "                from umap import UMAP\n",
        "                reducer = UMAP(n_components=r,n_neighbors=max(2*2*(K+1),min_samples),metric=metric)#,random_state=0,)\n",
        "                X = reducer.fit_transform(M)\n",
        "                if verbeux :\n",
        "                    print(\"Dimension réduite par UMAP :\\n d → r : \",d,\"→\",r)\n",
        "\n",
        "\n",
        "    faces_raw, e_u, e_v, e_w, nS = _build_graph_KSimplexes(X,K,min_samples,metric,complex_chosen,expZ,precision,verbeux)\n",
        "    if verbeux:\n",
        "        print(f\"{K}-simplices={nS}\")\n",
        "\n",
        "    if not faces_raw:\n",
        "        if K > d :\n",
        "            print(\"Warning: K too high compared to the dimension of the data. No clustering possible with such a K.\")\n",
        "        if return_multi_clusters :\n",
        "            return np.full(n, -1, dtype=np.int64), [(-1,1,1)]*n\n",
        "        else :\n",
        "            return np.full(n, 0, dtype=np.int64) ## ATTENTION\n",
        "\n",
        "    faces_raw = np.asarray(faces_raw, dtype=np.int64, order=\"C\")\n",
        "    e_u = np.asarray(e_u, dtype=np.int64)\n",
        "    e_v = np.asarray(e_v, dtype=np.int64)\n",
        "    e_w = np.asarray(e_w, dtype=np.float64)\n",
        "\n",
        "    # [3] Dédup des faces + inverse\n",
        "    faces_unique, inv = np.unique(faces_raw, axis=0, return_inverse=True)\n",
        "    if verbeux:\n",
        "        print(f\"Faces uniques: {faces_unique.shape[0]} (compression {faces_raw.shape[0]}→{faces_unique.shape[0]})\")\n",
        "\n",
        "    u = inv[e_u]; v = inv[e_v]; w = e_w\n",
        "\n",
        "    # [4] Dédup des arêtes (min poids) en vectoriel\n",
        "    uu = np.minimum(u, v); vv = np.maximum(u, v)\n",
        "    order = np.lexsort((vv, uu))\n",
        "    uu = uu[order]; vv = vv[order]; ww = w[order]\n",
        "    change = np.r_[True, (uu[1:] != uu[:-1]) | (vv[1:] != vv[:-1])]\n",
        "    gidx = np.flatnonzero(change)\n",
        "    ww = np.minimum.reduceat(ww, gidx)\n",
        "    uu = uu[gidx]; vv = vv[gidx]\n",
        "    if verbeux:\n",
        "        print(f\"Arêtes uniques (u<v): {uu.size} (avant dédup {u.size})\")\n",
        "\n",
        "    # [5] MST des faces via Kruskal + UnionFind\n",
        "    UFf = UnionFind(faces_unique.shape[0])\n",
        "    mst_faces_sorted = _kruskal_mst_from_edges(faces_unique.shape[0], uu, vv, ww, UFf)\n",
        "    if verbeux:\n",
        "        m = faces_unique.shape[0]; e_mst = len(mst_faces_sorted)\n",
        "        comps = max(0, m - e_mst) if m else 0\n",
        "        print(f\"MST faces: {e_mst} arêtes, composantes estimées: {comps}\")\n",
        "\n",
        "    # 6) Clustering via composants/occurrences optimisés (fonctions existantes inchangées ailleurs)\n",
        "    labels_points_unique, labels_points_multiple = build_Z_mst_occurrences_components(\n",
        "        faces_unique, mst_faces_sorted,\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        verbose=verbeux,\n",
        "        distinct_mode=\"owner\",\n",
        "        DBSCAN_threshold=DBSCAN_threshold\n",
        "    )\n",
        "\n",
        "    for i in range(n) :\n",
        "        labels_i = labels_points_multiple[i]\n",
        "        if len(labels_i) > 0 :\n",
        "            for label, prop, prob, *_ in labels_i :\n",
        "                if prop > 1/2 and prob > 1/2 :\n",
        "                    labels_points_unique[i] = label\n",
        "                    break\n",
        "    labels_points_unique = np.asarray(labels_points_unique)\n",
        "\n",
        "    def knn_fill_weighted(X, labels, k):\n",
        "        from sklearn.neighbors import KNeighborsClassifier\n",
        "        X = np.asarray(X); y = np.asarray(labels).copy()\n",
        "        mask_u = (y == -1);\n",
        "        if not mask_u.any(): return y\n",
        "        mask_l = ~mask_u\n",
        "        if not mask_l.any(): return y\n",
        "        k = min(k, mask_l.sum())\n",
        "        clf = KNeighborsClassifier(n_neighbors=k, weights=\"distance\", n_jobs=-1)\n",
        "        clf.fit(X[mask_l], y[mask_l])\n",
        "        y[mask_u] = clf.predict(X[mask_u])\n",
        "        return y\n",
        "\n",
        "    if label_all_points and Delaunay_possible :\n",
        "        labels_points_unique = knn_fill_weighted(M, labels_points_unique, min_samples)\n",
        "\n",
        "    if return_multi_clusters :\n",
        "        return labels_points_unique, labels_points_multiple\n",
        "    else :\n",
        "        return labels_points_unique"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}