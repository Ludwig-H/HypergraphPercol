{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation de HypergraphPercol"
      ],
      "metadata": {
        "id": "JS4m7zqZubam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Colab: paquets système pour CGAL/TBB/CMake\n",
        "apt-get update -qq\n",
        "apt-get install -y -qq build-essential cmake libcgal-dev libtbb-dev libtbbmalloc2 \\\n",
        "    libgmp-dev libmpfr-dev libeigen3-dev"
      ],
      "metadata": {
        "id": "aeK58mkguVvf",
        "outputId": "d30d47ca-ecb4-48dc-9b91-b8dd91097b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libgmpxx4ldbl:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 125080 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libgmpxx4ldbl_2%3a6.2.1+dfsg-3ubuntu1_amd64.deb ...\r\n",
            "Unpacking libgmpxx4ldbl:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\r\n",
            "Selecting previously unselected package libgmp-dev:amd64.\r\n",
            "Preparing to unpack .../libgmp-dev_2%3a6.2.1+dfsg-3ubuntu1_amd64.deb ...\r\n",
            "Unpacking libgmp-dev:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\r\n",
            "Selecting previously unselected package libmpfr-dev:amd64.\r\n",
            "Preparing to unpack .../libmpfr-dev_4.1.0-3build3_amd64.deb ...\r\n",
            "Unpacking libmpfr-dev:amd64 (4.1.0-3build3) ...\r\n",
            "Selecting previously unselected package libcgal-dev:amd64.\r\n",
            "Preparing to unpack .../libcgal-dev_5.4-1_amd64.deb ...\r\n",
            "Unpacking libcgal-dev:amd64 (5.4-1) ...\r\n",
            "Selecting previously unselected package libeigen3-dev.\r\n",
            "Preparing to unpack .../libeigen3-dev_3.4.0-2ubuntu2_all.deb ...\r\n",
            "Unpacking libeigen3-dev (3.4.0-2ubuntu2) ...\r\n",
            "Setting up libgmpxx4ldbl:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\r\n",
            "Setting up libeigen3-dev (3.4.0-2ubuntu2) ...\r\n",
            "Setting up libgmp-dev:amd64 (2:6.2.1+dfsg-3ubuntu1) ...\r\n",
            "Setting up libmpfr-dev:amd64 (4.1.0-3build3) ...\r\n",
            "Setting up libcgal-dev:amd64 (5.4-1) ...\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\r\n",
            "\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade pip setuptools wheel Cython cmake jedi"
      ],
      "metadata": {
        "id": "Jt8cOxlvuX1v",
        "outputId": "2758dbe5-e620-4fcc-c7ea-399f7c3687dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "mkdir -p \"${WORKDIR}\"\n",
        "cd \"${WORKDIR}\"\n",
        "if [ -d HypergraphPercol ]; then\n",
        "    git -C HypergraphPercol pull --ff-only\n",
        "else\n",
        "    git clone https://github.com/Ludwig-H/HypergraphPercol.git\n",
        "fi\n",
        "if [ -d cyminiball ]; then\n",
        "    git -C cyminiball pull --ff-only\n",
        "else\n",
        "    git clone https://github.com/Ludwig-H/cyminiball.git\n",
        "fi\n"
      ],
      "metadata": {
        "id": "NLiVDYDsuX9J",
        "outputId": "d5976254-b7e9-4e90-f4d7-a43fc0c5e054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'HypergraphPercol'...\n",
            "Cloning into 'cyminiball'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "mkdir -p \"${WORKDIR}/wheels\"\n",
        "cd \"${WORKDIR}/cyminiball\"\n",
        "python3 -m pip wheel --no-build-isolation --no-deps --wheel-dir=\"${WORKDIR}/wheels\" .\n",
        "python3 -m pip install --force-reinstall --no-deps --no-index --find-links=\"${WORKDIR}/wheels\" cyminiball\n",
        "# Le \"--no-deps\" indispensable pour que numpy ne se télécharge pas en version 2.3.4, créant des problèmes de compatibilité...\n"
      ],
      "metadata": {
        "id": "bl579jqLuYAG",
        "outputId": "f04a83f8-afea-4c97-b299-4b890a9bf214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/cyminiball\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: cyminiball\n",
            "  Building wheel for cyminiball (pyproject.toml): started\n",
            "  Building wheel for cyminiball (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for cyminiball: filename=cyminiball-2.1.1-cp312-cp312-linux_x86_64.whl size=771317 sha256=8565c0ba40bc6901cffa98a90ac322b28dc1a851593b57e3a4473cf6a6812a64\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h8g5c11g/wheels/54/ec/96/a7be7bb366d5f53158d4d263d66c218ba6e09a61d61b006edf\n",
            "Successfully built cyminiball\n",
            "Looking in links: /content/wheels\n",
            "Processing /content/wheels/cyminiball-2.1.1-cp312-cp312-linux_x86_64.whl\n",
            "Installing collected packages: cyminiball\n",
            "Successfully installed cyminiball-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "cd \"${WORKDIR}/HypergraphPercol\"\n",
        "python3 scripts/setup_cgal.py\n"
      ],
      "metadata": {
        "id": "OJn6BRqluYC_",
        "outputId": "edbd4ea4-1b90-48a7-b518-34ba548ab433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[clone] https://github.com/Ludwig-H/EdgesCGALWeightedDelaunay3D.git -> /content/HypergraphPercol/CGALDelaunay/EdgesCGALWeightedDelaunay3D\n",
            "[clone] https://github.com/Ludwig-H/EdgesCGALWeightedDelaunay2D.git -> /content/HypergraphPercol/CGALDelaunay/EdgesCGALWeightedDelaunay2D\n",
            "[clone] https://github.com/Ludwig-H/EdgesCGALWeightedDelaunayND.git -> /content/HypergraphPercol/CGALDelaunay/EdgesCGALWeightedDelaunayND\n",
            "[clone] https://github.com/Ludwig-H/EdgesCGALDelaunay3D.git -> /content/HypergraphPercol/CGALDelaunay/EdgesCGALDelaunay3D\n",
            "[clone] https://github.com/Ludwig-H/EdgesCGALDelaunay2D.git -> /content/HypergraphPercol/CGALDelaunay/EdgesCGALDelaunay2D\n",
            "[clone] https://github.com/Ludwig-H/EdgesCGALDelaunayND.git -> /content/HypergraphPercol/CGALDelaunay/EdgesCGALDelaunayND\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into '/content/HypergraphPercol/CGALDelaunay/EdgesCGALWeightedDelaunay3D'...\n",
            "Cloning into '/content/HypergraphPercol/CGALDelaunay/EdgesCGALWeightedDelaunay2D'...\n",
            "Cloning into '/content/HypergraphPercol/CGALDelaunay/EdgesCGALWeightedDelaunayND'...\n",
            "Cloning into '/content/HypergraphPercol/CGALDelaunay/EdgesCGALDelaunay3D'...\n",
            "Cloning into '/content/HypergraphPercol/CGALDelaunay/EdgesCGALDelaunay2D'...\n",
            "Cloning into '/content/HypergraphPercol/CGALDelaunay/EdgesCGALDelaunayND'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "cd \"${WORKDIR}/HypergraphPercol/CGALDelaunay\"\n",
        "\n",
        "projects=(\n",
        "    EdgesCGALDelaunay2D\n",
        "    EdgesCGALDelaunay3D\n",
        "    EdgesCGALDelaunayND\n",
        "    EdgesCGALWeightedDelaunay2D\n",
        "    EdgesCGALWeightedDelaunay3D\n",
        "    EdgesCGALWeightedDelaunayND\n",
        ")\n",
        "\n",
        "for project in \"${projects[@]}\"; do\n",
        "    cmake -S \"${project}\" -B \"${project}/build\" -DCMAKE_BUILD_TYPE=Release\n",
        "    cmake --build \"${project}/build\" --config Release\n",
        "    cmake --install \"${project}/build\" --prefix \"${WORKDIR}/HypergraphPercol\"\n",
        "done\n"
      ],
      "metadata": {
        "id": "K4Jt04NKuYFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "cd \"${WORKDIR}/HypergraphPercol\"\n",
        "python3 -m pip install --no-deps --force-reinstall .\n"
      ],
      "metadata": {
        "id": "6onzntiIuYIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "workdir = os.environ.get(\"HGP_WORKDIR\", \"/content\")\n",
        "repo_root = os.path.join(workdir, \"HypergraphPercol\")\n",
        "os.environ[\"CGALDELAUNAY_ROOT\"] = os.path.join(repo_root, \"CGALDelaunay\")\n",
        "\n",
        "from hypergraphpercol import HypergraphPercol"
      ],
      "metadata": {
        "id": "z5hYAD5zuYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application de HypergraphPercol aux *reads* Nanopore"
      ],
      "metadata": {
        "id": "Ey2S3Dhsu1WJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ow18ktSxR4n"
      },
      "outputs": [],
      "source": [
        "# @title Setup: dépendances et données\n",
        "!pip -q install datasketch edlib numpy pandas scipy scikit-learn tqdm pyarrow\n",
        "\n",
        "import os, sys, re, math, random, itertools, json\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Raw URLs du dépôt Microsoft (dataset seul)\n",
        "CENTERS_URL  = \"https://raw.githubusercontent.com/microsoft/clustered-nanopore-reads-dataset/main/Centers.txt\"\n",
        "CLUSTERS_URL = \"https://raw.githubusercontent.com/microsoft/clustered-nanopore-reads-dataset/main/Clusters.txt\"\n",
        "\n",
        "def download(url, dst):\n",
        "    import urllib.request\n",
        "    if not Path(dst).exists():\n",
        "        print(f\"Téléchargement {url} -> {dst}\")\n",
        "        urllib.request.urlretrieve(url, dst)\n",
        "    else:\n",
        "        print(f\"Déjà présent: {dst}\")\n",
        "\n",
        "download(CENTERS_URL,  DATA_DIR/\"Centers.txt\")\n",
        "download(CLUSTERS_URL, DATA_DIR/\"Clusters.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parsing des fichiers { vertical-output: true }\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Tuple, Dict, Iterable, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def parse_clusters(clusters_path: Path) -> Tuple[List[str], np.ndarray]:\n",
        "    reads = []\n",
        "    labels = []\n",
        "    label  = -1\n",
        "    with open(clusters_path, \"r\") as f:\n",
        "        for raw in f:\n",
        "            s = raw.strip()\n",
        "            if not s:\n",
        "                continue\n",
        "            if s.startswith(\"=\"):           # lignes de séparation \"====\"\n",
        "                label += 1\n",
        "                continue\n",
        "            reads.append(s)\n",
        "            labels.append(label)\n",
        "    return reads, np.array(labels, dtype=np.int32)\n",
        "\n",
        "reads, labels = parse_clusters(DATA_DIR/\"Clusters.txt\")\n",
        "n_reads = len(reads)\n",
        "n_clusters_seen = labels.max() + 1\n",
        "print(f\"{n_reads} reads, {n_clusters_seen} clusters (certains peuvent être vides).\")\n",
        "\n",
        "# Option: charger Centers pour évaluation ultérieure\n",
        "centers = [line.strip() for line in open(DATA_DIR/'Centers.txt') if line.strip()]\n",
        "print(f\"{len(centers)} centers de longueur attendue 110.\")\n"
      ],
      "metadata": {
        "id": "i9VWZdy4xb4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from collections import Counter\n",
        "import edlib, random, math\n",
        "\n",
        "# ------------------------------\n",
        "# Utilitaires d'alignement edlib\n",
        "# ------------------------------\n",
        "\n",
        "def _cigar_to_gapped(a: str, b: str, cigar: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Convertit un CIGAR edlib (=,X,I,D) en deux chaînes gappées de même longueur.\n",
        "    '=' : match, 'X' : mismatch, 'I' : insertion dans la cible (gap côté a), 'D' : deletion (gap côté b).\n",
        "    On tolère 'M' au cas où.\n",
        "    \"\"\"\n",
        "    ia = ib = 0\n",
        "    A, B = [], []\n",
        "    num = 0\n",
        "\n",
        "    for ch in cigar:\n",
        "        if ch.isdigit():\n",
        "            num = num * 10 + int(ch)\n",
        "            continue\n",
        "        cnt = num if num > 0 else 1\n",
        "        num = 0\n",
        "\n",
        "        if ch in ('=', 'M', 'X'):\n",
        "            for _ in range(cnt):\n",
        "                ca = a[ia] if ia < len(a) else '-'\n",
        "                cb = b[ib] if ib < len(b) else '-'\n",
        "                A.append(ca)\n",
        "                B.append(cb)\n",
        "                if ia < len(a): ia += 1\n",
        "                if ib < len(b): ib += 1\n",
        "        elif ch == 'I':\n",
        "            for _ in range(cnt):\n",
        "                A.append('-')\n",
        "                cb = b[ib] if ib < len(b) else '-'\n",
        "                B.append(cb)\n",
        "                if ib < len(b): ib += 1\n",
        "        elif ch == 'D':\n",
        "            for _ in range(cnt):\n",
        "                ca = a[ia] if ia < len(a) else '-'\n",
        "                A.append(ca)\n",
        "                B.append('-')\n",
        "                if ia < len(a): ia += 1\n",
        "        else:\n",
        "            raise ValueError(f\"Opérateur CIGAR non géré: {ch}\")\n",
        "\n",
        "    L = max(len(A), len(B))\n",
        "    if len(A) < L: A.extend(['-'] * (L - len(A)))\n",
        "    if len(B) < L: B.extend(['-'] * (L - len(B)))\n",
        "    return ''.join(A), ''.join(B)\n",
        "\n",
        "\n",
        "def global_align_edlib(a: str, b: str) -> Tuple[str, str]:\n",
        "    \"\"\"Alignement global simple via edlib (orientation imposée).\"\"\"\n",
        "    res = edlib.align(a, b, mode=\"NW\", task=\"path\")\n",
        "    A, B = _cigar_to_gapped(a, b, res[\"cigar\"])\n",
        "    return A, B\n",
        "\n",
        "\n",
        "def global_align_edlib_best_oriented(a: str, b: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Alignement global en choisissant automatiquement l’orientation de b (directe ou renversée)\n",
        "    qui minimise la distance d’édition.\n",
        "    \"\"\"\n",
        "    res_dir = edlib.align(a, b, mode=\"NW\", task=\"path\")\n",
        "    d_dir = res_dir[\"editDistance\"]\n",
        "\n",
        "    b_rev = b[::-1]\n",
        "    res_rev = edlib.align(a, b_rev, mode=\"NW\", task=\"path\")\n",
        "    d_rev = res_rev[\"editDistance\"]\n",
        "\n",
        "    if d_rev < d_dir:\n",
        "        return _cigar_to_gapped(a, b_rev, res_rev[\"cigar\"])\n",
        "    else:\n",
        "        return _cigar_to_gapped(a, b, res_dir[\"cigar\"])\n",
        "\n",
        "\n",
        "def _bidir_edit(a: str, b: str) -> Tuple[int, bool]:\n",
        "    \"\"\"\n",
        "    Distance d’édition globale min{ dist(a,b), dist(a,rev(b)) }.\n",
        "    Retourne (meilleure_distance, used_reversed).\n",
        "    \"\"\"\n",
        "    d = edlib.align(a, b, mode=\"NW\", task=\"distance\")[\"editDistance\"]\n",
        "    d_rev = edlib.align(a, b[::-1], mode=\"NW\", task=\"distance\")[\"editDistance\"]\n",
        "    if d_rev < d:\n",
        "        return d_rev, True\n",
        "    return d, False\n",
        "\n",
        "\n",
        "def norm_edit(a: str, b: str) -> float:\n",
        "    \"\"\"\n",
        "    Distance d'édition normalisée, en autorisant le renversement.\n",
        "    \"\"\"\n",
        "    best, _ = _bidir_edit(a, b)\n",
        "    return best / max(1, max(len(a), len(b)))\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# Sélection du méduloïde (médoïde lite) — bi-orientation\n",
        "# -----------------------------------\n",
        "\n",
        "def medoid_index(read_ids: List[int], reads: List[str], sample_size: int = 200, band: int = 20) -> int:\n",
        "    \"\"\"\n",
        "    Retourne l'indice (dans read_ids) du read qui minimise la somme des distances aux autres\n",
        "    (sur un échantillon), en autorisant le renversement au niveau pairwise.\n",
        "    Paramètre 'band' ignoré: on utilise uniquement edlib en global.\n",
        "    \"\"\"\n",
        "    if len(read_ids) == 1:\n",
        "        return 0\n",
        "\n",
        "    sample = read_ids if len(read_ids) <= sample_size else random.sample(read_ids, sample_size)\n",
        "\n",
        "    best_tot, best_r = float('inf'), None\n",
        "    for r in sample:\n",
        "        tot = 0.0\n",
        "        ar = reads[r]\n",
        "        for s in sample:\n",
        "            if s == r:\n",
        "                continue\n",
        "            # distance bi-orientée (direct vs renversée)\n",
        "            d_rs, _ = _bidir_edit(ar, reads[s])\n",
        "            tot += d_rs / max(1, max(len(ar), len(reads[s])))\n",
        "        if tot < best_tot:\n",
        "            best_tot, best_r = tot, r\n",
        "\n",
        "    return read_ids.index(best_r)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Ajustement à longueur 110 (inchangé)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "def adjust_to_length_110(draft: str, aligned_reads: List[str], aligned_draft: List[str], target_len: int = 110) -> str:\n",
        "    if not aligned_draft or not aligned_reads:\n",
        "        return draft[:target_len].ljust(target_len, 'A')\n",
        "\n",
        "    L = max(max(len(s) for s in aligned_draft), max(len(s) for s in aligned_reads))\n",
        "\n",
        "    col_stats = []\n",
        "    for c in range(L):\n",
        "        draft_chars = [ad[c] if c < len(ad) else '-' for ad in aligned_draft]\n",
        "        read_chars  = [ar[c] if c < len(ar) else '-' for ar in aligned_reads]\n",
        "\n",
        "        ins  = sum(1 for dch, rch in zip(draft_chars, read_chars) if dch == '-' and rch != '-')\n",
        "        dele = sum(1 for dch, rch in zip(draft_chars, read_chars) if dch != '-' and rch == '-')\n",
        "\n",
        "        cnt = Counter([rch for rch in read_chars if rch != '-'])\n",
        "        if cnt:\n",
        "            maj_base, maj_count = cnt.most_common(1)[0]\n",
        "            tot = sum(cnt.values())\n",
        "            probs = [v / max(1, tot) for v in cnt.values()]\n",
        "            entropy = -sum(p * math.log2(p + 1e-12) for p in probs)\n",
        "        else:\n",
        "            maj_base, maj_count, entropy = ('A', 0, 0.0)\n",
        "\n",
        "        col_stats.append({\"ins\": ins, \"del\": dele, \"maj\": maj_base, \"majc\": maj_count, \"entropy\": entropy})\n",
        "\n",
        "    ref_aligned_draft = aligned_draft[0] if aligned_draft else \"-\" * L\n",
        "    if len(ref_aligned_draft) < L:\n",
        "        ref_aligned_draft = ref_aligned_draft + \"-\" * (L - len(ref_aligned_draft))\n",
        "\n",
        "    cols = []\n",
        "    thr_major = max(2, len(aligned_reads) // 3)\n",
        "    for c in range(L):\n",
        "        if ref_aligned_draft[c] == '-':\n",
        "            cols.append(col_stats[c][\"maj\"] if col_stats[c][\"majc\"] > 0 else 'A')\n",
        "        else:\n",
        "            base = ref_aligned_draft[c]\n",
        "            if col_stats[c][\"majc\"] >= thr_major:\n",
        "                base = col_stats[c][\"maj\"]\n",
        "            cols.append(base)\n",
        "\n",
        "    if len(cols) > target_len:\n",
        "        order = sorted(range(len(cols)), key=lambda c: (col_stats[c][\"del\"], col_stats[c][\"entropy\"]), reverse=True)\n",
        "        to_drop = set(order[:len(cols) - target_len])\n",
        "        cols = [cols[c] for c in range(len(cols)) if c not in to_drop]\n",
        "    elif len(cols) < target_len:\n",
        "        need = target_len - len(cols)\n",
        "        order = sorted(range(len(cols)), key=lambda c: col_stats[c][\"ins\"], reverse=True)\n",
        "        ins_positions = order[:max(1, min(need, len(order)))]\n",
        "        pos_iter = 0\n",
        "        while need > 0:\n",
        "            c = ins_positions[pos_iter % len(ins_positions)]\n",
        "            cols.insert(c, col_stats[c][\"maj\"] if col_stats[c][\"majc\"] > 0 else 'A')\n",
        "            need -= 1\n",
        "            pos_iter += 1\n",
        "\n",
        "    if len(cols) != target_len:\n",
        "        cols = (cols[:target_len] if len(cols) > target_len else cols + ['A'] * (target_len - len(cols)))\n",
        "    return ''.join(cols)\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# Calcul du \"center\" de cluster via consensus\n",
        "# -----------------------------------------\n",
        "\n",
        "def compute_cluster_center(read_ids: List[int], reads: List[str],\n",
        "                           target_len: int = 110, band: int = 20, max_reads_align: int = 400) -> str:\n",
        "    \"\"\"\n",
        "    Identique à ta version, mais les alignements pairwise se font en orientation optimale\n",
        "    (directe ou renversée) pour éviter de polluer le consensus.\n",
        "    \"\"\"\n",
        "    if not read_ids:\n",
        "        return \"\"\n",
        "\n",
        "    ids_for_align = read_ids if len(read_ids) <= max_reads_align else random.sample(read_ids, max_reads_align)\n",
        "\n",
        "    # médoïde (bi-orientation dans medoid_index)\n",
        "    midx_in_ids = medoid_index(ids_for_align, reads, sample_size=min(200, len(ids_for_align)), band=band)\n",
        "    medoid_read_id = ids_for_align[midx_in_ids]\n",
        "    draft = reads[medoid_read_id]\n",
        "\n",
        "    # alignement des reads sur le draft (orientation optimale)\n",
        "    aligned_draft, aligned_reads = [], []\n",
        "    for rid in ids_for_align:\n",
        "        A, B = global_align_edlib_best_oriented(draft, reads[rid])\n",
        "        L = max(len(A), len(B))\n",
        "        if len(A) < L: A += '-' * (L - len(A))\n",
        "        if len(B) < L: B += '-' * (L - len(B))\n",
        "        aligned_draft.append(A)\n",
        "        aligned_reads.append(B)\n",
        "\n",
        "    center = adjust_to_length_110(draft, aligned_reads, aligned_draft, target_len=target_len)\n",
        "\n",
        "    # raffinement: réaligner sur le center (orientation optimale)\n",
        "    aligned_center, aligned_reads2 = [], []\n",
        "    for rid in ids_for_align:\n",
        "        A, B = global_align_edlib_best_oriented(center, reads[rid])\n",
        "        L = max(len(A), len(B))\n",
        "        if len(A) < L: A += '-' * (L - len(A))\n",
        "        if len(B) < L: B += '-' * (L - len(B))\n",
        "        aligned_center.append(A)\n",
        "        aligned_reads2.append(B)\n",
        "\n",
        "    L = max(len(s) for s in aligned_center) if aligned_center else len(center)\n",
        "    aligned_center = [s.ljust(L, '-') for s in aligned_center]\n",
        "    aligned_reads2 = [s.ljust(L, '-') for s in aligned_reads2]\n",
        "\n",
        "    cols = []\n",
        "    for c in range(L):\n",
        "        cnt = Counter([ar[c] for ar in aligned_reads2 if c < len(ar) and ar[c] != '-'])\n",
        "        if cnt:\n",
        "            base = cnt.most_common(1)[0][0]\n",
        "        else:\n",
        "            base = center[c] if c < len(center) and center[c] != '-' else 'A'\n",
        "        cols.append(base)\n",
        "\n",
        "    final = ''.join([x for x in cols if x != '-'])\n",
        "    if len(final) != target_len:\n",
        "        final = (final[:target_len] if len(final) > target_len else final + 'A' * (target_len - len(final)))\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "BFd_YoJsV7C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LSH (direct + renversé) + Graphe sparse top-K (bi-orientation) { vertical-output: true }\n",
        "\n",
        "# Installs (au cas où)\n",
        "!pip -q install edlib datasketch joblib tqdm tqdm-joblib\n",
        "\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "from joblib import Parallel, delayed, cpu_count\n",
        "from tqdm.auto import tqdm\n",
        "try:\n",
        "    from tqdm_joblib import tqdm_joblib\n",
        "except ImportError:\n",
        "    tqdm_joblib = None\n",
        "\n",
        "import edlib, random, numpy as np\n",
        "\n",
        "# =========================\n",
        "# Hyperparamètres\n",
        "# =========================\n",
        "NUM_PERM      = 128    # taille du sketch MinHash\n",
        "KMER          = 8     # k-mer size pour ~110 nt\n",
        "LSH_THRESHOLD = 0.25   # seuil Jaccard approx -> candidats\n",
        "SEED          = 1337\n",
        "\n",
        "# Graphe\n",
        "TOP_K        = 30        # voisins conservés par nœud (après LSH)\n",
        "MAX_EDGES    = None      # limite dure sur le nb d'arêtes (None = pas de limite)\n",
        "NORMALIZE_BY = \"maxlen\"  # \"maxlen\" ou \"meanlen\"\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "# =========================\n",
        "# K-mers et MinHash helpers\n",
        "# =========================\n",
        "def kmers(seq: str, k: int):\n",
        "    L = len(seq)\n",
        "    if L < k:\n",
        "        return []\n",
        "    return [seq[i:i+k] for i in range(L - k + 1)]\n",
        "\n",
        "def _sketch_from_kmers(kmer_iter, num_perm=NUM_PERM, seed=SEED) -> MinHash:\n",
        "    m = MinHash(num_perm=num_perm, seed=seed)\n",
        "    for km in kmer_iter:\n",
        "        m.update(km.encode(\"utf8\"))\n",
        "    return m\n",
        "\n",
        "def _sketch_direct(seq: str, k=KMER, num_perm=NUM_PERM) -> MinHash:\n",
        "    return _sketch_from_kmers(kmers(seq, k), num_perm=num_perm, seed=SEED)\n",
        "\n",
        "def _sketch_reversed(seq: str, k=KMER, num_perm=NUM_PERM) -> MinHash:\n",
        "    rseq = seq[::-1]\n",
        "    return _sketch_from_kmers(kmers(rseq, k), num_perm=num_perm, seed=SEED)\n",
        "\n",
        "def build_two_sketches(reads, k=KMER, num_perm=NUM_PERM, n_jobs=None, batch_size=64):\n",
        "    \"\"\"\n",
        "    Construit deux ensembles de MinHash:\n",
        "      - direct_sketches[i]   = sketch du read i\n",
        "      - reversed_sketches[i] = sketch du read i renversé\n",
        "    \"\"\"\n",
        "    if n_jobs is None:\n",
        "        n_jobs = cpu_count()\n",
        "\n",
        "    if n_jobs == 1 or tqdm_joblib is None:\n",
        "        direct = [ _sketch_direct(s, k=k, num_perm=num_perm)   for s in tqdm(reads, desc=\"MinHash direct\",   unit=\"read\") ]\n",
        "        rev    = [ _sketch_reversed(s, k=k, num_perm=num_perm) for s in tqdm(reads, desc=\"MinHash reversed\", unit=\"read\") ]\n",
        "    else:\n",
        "        with tqdm_joblib(tqdm(total=len(reads), desc=\"MinHash direct (parallel)\", unit=\"read\")):\n",
        "            direct = Parallel(n_jobs=n_jobs, prefer=\"processes\", batch_size=batch_size)(\n",
        "                delayed(_sketch_direct)(s, k=k, num_perm=num_perm) for s in reads\n",
        "            )\n",
        "        with tqdm_joblib(tqdm(total=len(reads), desc=\"MinHash reversed (parallel)\", unit=\"read\")):\n",
        "            rev = Parallel(n_jobs=n_jobs, prefer=\"processes\", batch_size=batch_size)(\n",
        "                delayed(_sketch_reversed)(s, k=k, num_perm=num_perm) for s in reads\n",
        "            )\n",
        "    return direct, rev\n",
        "\n",
        "def build_lsh_direct_and_reversed(direct_sketches, reversed_sketches,\n",
        "                                  threshold=LSH_THRESHOLD, num_perm=NUM_PERM):\n",
        "    \"\"\"\n",
        "    Construit un index LSH contenant deux entrées par read:\n",
        "      - clé (i, 0) -> sketch direct[i]\n",
        "      - clé (i, 1) -> sketch reversed[i]\n",
        "    \"\"\"\n",
        "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "    for i, m in enumerate(direct_sketches):\n",
        "        lsh.insert((i, 0), m)\n",
        "    for i, m in enumerate(reversed_sketches):\n",
        "        lsh.insert((i, 1), m)\n",
        "    return lsh\n",
        "\n",
        "def lsh_query_both_for_index(lsh, direct_sketches, reversed_sketches, idx: int):\n",
        "    \"\"\"\n",
        "    Requête l'index pour un read existant (par son index) en bi-orientation.\n",
        "    Retourne un set d'identifiants de reads voisins (sans orientation), sans idx lui-même.\n",
        "    \"\"\"\n",
        "    cand = set()\n",
        "    for key in lsh.query(direct_sketches[idx]):\n",
        "        cand.add(key[0])\n",
        "    for key in lsh.query(reversed_sketches[idx]):\n",
        "        cand.add(key[0])\n",
        "    cand.discard(idx)\n",
        "    return cand\n",
        "\n",
        "# =========================\n",
        "# Distances d'édition\n",
        "# =========================\n",
        "def best_oriented_edit(a: str, b: str) -> int:\n",
        "    \"\"\"\n",
        "    Distance d’édition globale min{ dist(a,b), dist(a,rev(b)) } via edlib (NW).\n",
        "    \"\"\"\n",
        "    d1 = edlib.align(a, b,     mode=\"NW\", task=\"distance\")[\"editDistance\"]\n",
        "    d2 = edlib.align(a, b[::-1], mode=\"NW\", task=\"distance\")[\"editDistance\"]\n",
        "    return d1 if d1 <= d2 else d2\n",
        "\n",
        "def norm_edit(a: str, b: str, how=NORMALIZE_BY) -> float:\n",
        "    \"\"\"\n",
        "    Distance normalisée d’édition bi-orientation.\n",
        "    how = 'maxlen' ou 'meanlen'\n",
        "    \"\"\"\n",
        "    d = best_oriented_edit(a, b)\n",
        "    if how == \"meanlen\":\n",
        "        denom = (len(a) + len(b)) / 2.0\n",
        "    else:\n",
        "        denom = max(len(a), len(b))\n",
        "    return d / max(1.0, denom)\n",
        "\n",
        "# =========================\n",
        "# Build sketches + LSH\n",
        "# =========================\n",
        "# 'reads' doit exister: liste[str] des séquences\n",
        "direct_sketches, reversed_sketches = build_two_sketches(\n",
        "    reads, k=KMER, num_perm=NUM_PERM, n_jobs=None, batch_size=64\n",
        ")\n",
        "lsh = build_lsh_direct_and_reversed(direct_sketches, reversed_sketches,\n",
        "                                    threshold=LSH_THRESHOLD, num_perm=NUM_PERM)\n",
        "\n",
        "print(\"Index LSH construit (direct + renversé).\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D-bHHJoAYsqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Construction du graphe\n",
        "# =========================\n",
        "edges = []   # liste de tuples (i, j, w_edit_norm, jacc_max)\n",
        "edge_count = 0\n",
        "\n",
        "# utilitaire: score Jaccard approx bi-orientation pour prioriser\n",
        "def jaccard_biorient(i: int, j: int) -> float:\n",
        "    # on prend le max entre les 4 combinaisons plausibles\n",
        "    sdd = direct_sketches[i].jaccard(direct_sketches[j])\n",
        "    sdr = direct_sketches[i].jaccard(reversed_sketches[j])\n",
        "    srd = reversed_sketches[i].jaccard(direct_sketches[j])\n",
        "    srr = reversed_sketches[i].jaccard(reversed_sketches[j])\n",
        "    return max(sdd, sdr, srd, srr)\n",
        "\n",
        "with tqdm(total=len(reads), desc=\"Voisins + distances\", unit=\"node\") as pbar:\n",
        "    for i in range(len(reads)):\n",
        "        # Candidats bi-orientation via LSH\n",
        "        cand = list(lsh_query_both_for_index(lsh, direct_sketches, reversed_sketches, i))\n",
        "        if not cand:\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "        # Scoring approx (Jaccard) pour prioriser les plus proches\n",
        "        scored = [(j, jaccard_biorient(i, j)) for j in cand]\n",
        "        scored.sort(key=lambda t: t[1], reverse=True)\n",
        "        keep = scored[:TOP_K] if TOP_K and TOP_K > 0 else scored\n",
        "\n",
        "        # Rerank local par vraie distance d'édition bi-orientation\n",
        "        for j, jac in keep:\n",
        "            # if j <= i:\n",
        "            #     # on ne crée l’arête (i,j) qu’une seule fois (graphe non orienté)\n",
        "            #     continue\n",
        "            d_norm = norm_edit(reads[i], reads[j], how=NORMALIZE_BY)\n",
        "            edges.append((min(i,j), max(i, j), float(d_norm), float(jac)))\n",
        "            edge_count += 1\n",
        "            if MAX_EDGES and edge_count >= MAX_EDGES:\n",
        "                break\n",
        "\n",
        "        if MAX_EDGES and edge_count >= MAX_EDGES:\n",
        "            break\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"{len(edges)} arêtes construites (non orientées, parallèle, mémoire bornée).\")"
      ],
      "metadata": {
        "id": "aUkKLxbu4UFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HypergraphPercol"
      ],
      "metadata": {
        "id": "eOUo9qSSSe0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gudhi\n",
        "\n",
        "M = [(min(i,j),max(i,j),d) for i,j,d,_ in edges]\n",
        "K = 2\n",
        "min_cluster_size = 3\n",
        "metric = \"sparse\"\n",
        "expZ=2\n",
        "verbeux = True\n",
        "\n",
        "labels_pred = HypergraphPercol(M=M,\n",
        "                               K=K,\n",
        "                               expZ=expZ,\n",
        "                               min_cluster_size=min_cluster_size,\n",
        "                               metric=metric,\n",
        "                               verbeux=verbeux)"
      ],
      "metadata": {
        "id": "KLT6H-BVSeOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pred_clusters = sorted([int(c) for c in set(labels_pred) if c != -1])\n",
        "print(\"Nb clusters prédits (sans -1):\", len(pred_clusters))\n",
        "\n",
        "cluster_to_readids = defaultdict(list)\n",
        "for rid, c in enumerate(labels_pred):\n",
        "    c = int(c)\n",
        "    if c != -1:\n",
        "        cluster_to_readids[c].append(rid)\n",
        "\n",
        "pred_centers = {}\n",
        "for c in tqdm(pred_clusters, desc=\"Centers by cluster\"):\n",
        "    c = int(c)\n",
        "    read_ids = cluster_to_readids[c]\n",
        "    pred_centers[c] = compute_cluster_center(read_ids, reads, target_len=110, band=20, max_reads_align=400)\n",
        "\n",
        "rows = [{\"pred_cluster\": c, \"pred_center\": s} for c, s in pred_centers.items()]\n",
        "# pd.DataFrame(rows).to_csv(CENTERS_CSV, index=False)\n",
        "# print(\"Saved predicted centers:\", CENTERS_CSV)\n"
      ],
      "metadata": {
        "id": "63eAehvMTpFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(pred_clusters)\n",
        "cluster_to_readids\n",
        "pred_centers"
      ],
      "metadata": {
        "id": "kzcuqsbvriWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ===========================\n",
        "# Metrics sur centers (edlib)\n",
        "# Parallelisé avec joblib + tqdm_joblib\n",
        "# Matching et stats bi-orientation (direct / reversed)\n",
        "# ===========================\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import edlib\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm.auto import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Distances via edlib\n",
        "# ----------------------------\n",
        "def _ed_mode(a: str, b: str, mode: str = \"NW\") -> int:\n",
        "    \"\"\"Distance d’édition via edlib pour un mode donné: 'NW' (global) ou 'HW' (semi-global).\"\"\"\n",
        "    return edlib.align(a, b, mode=mode, task=\"distance\")[\"editDistance\"]\n",
        "\n",
        "def _best_bidorient(a: str, b: str, mode: str = \"NW\") -> Tuple[int, bool, int, int]:\n",
        "    \"\"\"\n",
        "    Retourne (best_dist, used_reversed, dist_direct, dist_reversed)\n",
        "    où best_dist = min(dist(a,b), dist(a,b[::-1])) sur le mode edlib demandé.\n",
        "    \"\"\"\n",
        "    d_dir = _ed_mode(a, b, mode=mode)\n",
        "    d_rev = _ed_mode(a, b[::-1], mode=mode)\n",
        "    if d_rev < d_dir:\n",
        "        return d_rev, True, d_dir, d_rev\n",
        "    else:\n",
        "        return d_dir, False, d_dir, d_rev\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Passes parallèles: meilleur voisin de l'autre côté\n",
        "#   Matching fait sur distance globale (NW) en bi-orientation\n",
        "# ----------------------------------------------------\n",
        "def _best_true_for_one_pred(pseq: str, centers: List[str]) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Calcule le meilleur true center (index) pour un center prédit donné,\n",
        "    en minimisant la distance d'édition GLOBALE (NW) sur {direct, reversed}.\n",
        "    Retourne (true_idx, best_dist_NW).\n",
        "    \"\"\"\n",
        "    best_i, best_d = None, math.inf\n",
        "    for i, tseq in enumerate(centers):\n",
        "        d, _, _, _ = _best_bidorient(pseq, tseq, mode=\"NW\")\n",
        "        if d < best_d:\n",
        "            best_d, best_i = d, i\n",
        "    return best_i, int(best_d)\n",
        "\n",
        "\n",
        "def _best_pred_for_one_true(tseq: str, pred_items: List[Tuple[int, str]]) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Symétrique: pour un true center donné, retourne (pred_key, best_dist_NW) en bi-orientation.\n",
        "    \"\"\"\n",
        "    best_pk, best_d = None, math.inf\n",
        "    for pk, pseq in pred_items:\n",
        "        d, _, _, _ = _best_bidorient(pseq, tseq, mode=\"NW\")\n",
        "        if d < best_d:\n",
        "            best_d, best_pk = d, pk\n",
        "    return best_pk, int(best_d)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Matching glouton 1-1 par distance croissante (NW)\n",
        "# ----------------------------------------------------\n",
        "def _greedy_match_from_candidates(candidates: List[Tuple[int, int, int]],\n",
        "                                  n_true: int,\n",
        "                                  threshold: int = None) -> List[Tuple[int, int, int]]:\n",
        "    \"\"\"\n",
        "    candidates: liste de (dist_NW, pred_key, true_idx)\n",
        "    Retourne une liste de paires (true_idx, pred_key, dist_NW) appariées 1-1.\n",
        "    \"\"\"\n",
        "    candidates.sort(key=lambda t: t[0])  # dist croissante\n",
        "    used_true, used_pred = set(), set()\n",
        "    pairs = []\n",
        "    for d, pk, ti in candidates:\n",
        "        if threshold is not None and d > threshold:\n",
        "            continue\n",
        "        if pk in used_pred or ti in used_true:\n",
        "            continue\n",
        "        used_pred.add(pk)\n",
        "        used_true.add(ti)\n",
        "        pairs.append((ti, pk, d))\n",
        "        if len(used_true) == n_true:\n",
        "            break\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Fonction principale\n",
        "# ----------------------------------------------------\n",
        "def compute_center_metrics_parallel(centers: List[str],\n",
        "                                    pred_centers: Dict[int, str],\n",
        "                                    THRESHOLD_PROX: int = None,\n",
        "                                    n_jobs: int = -1) -> Tuple[dict, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Calcule des métriques de pertinence des centers prédits, en parallèle.\n",
        "    Matching: distance d'édition GLOBALE (NW) avec inversion autorisée.\n",
        "    Statistiques: sur les paires retenues, calcule aussi les distances HW (semi-global),\n",
        "                  toujours en bi-orientation.\n",
        "    Sorties:\n",
        "      - summary: dict de métriques globales\n",
        "      - df_pairs: DataFrame détaillée des appariements (d_NW/HW direct/rev, orientation choisie)\n",
        "      - df_mintrue: min distance NW (bi-orientation) pour chaque true vers tous les pred\n",
        "    \"\"\"\n",
        "\n",
        "    n_true = len(centers)\n",
        "    pred_items = sorted(pred_centers.items())  # liste [(pred_key, seq), ...]\n",
        "    n_pred = len(pred_items)\n",
        "\n",
        "    # Cas trivial\n",
        "    if n_true == 0 and n_pred == 0:\n",
        "        return {\n",
        "            \"n_true_centers\": 0, \"n_pred_centers\": 0,\n",
        "            \"classes_retrouvees\": 0, \"precision_classes\": 0.0,\n",
        "            \"recall_classes\": 0.0, \"f1_classes\": 0.0, \"classes_en_trop\": 0,\n",
        "            \"avg_editdist_on_matched_NW\": None, \"median_editdist_on_matched_NW\": None,\n",
        "            \"p90_editdist_on_matched_NW\": None, \"max_editdist_on_matched_NW\": None,\n",
        "            \"exact_match_rate_on_matched_NW\": 0.0,\n",
        "            \"avg_editdist_on_matched_HW\": None, \"median_editdist_on_matched_HW\": None,\n",
        "            \"p90_editdist_on_matched_HW\": None, \"max_editdist_on_matched_HW\": None,\n",
        "            \"exact_match_rate_on_matched_HW\": 0.0,\n",
        "            \"count_dist_0_NW\": 0, \"count_dist_1_NW\": 0, \"count_dist_2_NW\": 0, \"count_dist_3_NW\": 0,\n",
        "            \"count_dist_0_HW\": 0, \"count_dist_1_HW\": 0, \"count_dist_2_HW\": 0, \"count_dist_3_HW\": 0,\n",
        "        }, pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # -------------------------------\n",
        "    # Passe 1: pour chaque prédict, meilleur TRUE (NW bi-orient)\n",
        "    # -------------------------------\n",
        "    with tqdm_joblib(tqdm(desc=\"Best TRUE for each PRED (NW, bi-orient)\", total=n_pred)) as _:\n",
        "        best_true_list = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
        "            delayed(_best_true_for_one_pred)(pseq, centers) for _, pseq in pred_items\n",
        "        )\n",
        "    best_true_for_pred_map = {pk: res for (pk, _), res in zip(pred_items, best_true_list)}\n",
        "    min_pred_over_trues_NW = np.array([d for (_, d) in best_true_list], dtype=float)  # par pred\n",
        "\n",
        "    # Fabrication des candidats (dist_NW, pred_key, true_idx) à partir de la passe 1\n",
        "    candidates = [(best_true_for_pred_map[pk][1], pk, best_true_for_pred_map[pk][0])\n",
        "                  for pk, _ in pred_items if best_true_for_pred_map[pk][0] is not None]\n",
        "\n",
        "    # -------------------------------\n",
        "    # Passe 2: pour chaque TRUE, meilleur PRED (NW bi-orient)\n",
        "    # -------------------------------\n",
        "    with tqdm_joblib(tqdm(desc=\"Best PRED for each TRUE (NW, bi-orient)\", total=n_true)) as _:\n",
        "        best_pred_list = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
        "            delayed(_best_pred_for_one_true)(tseq, pred_items) for tseq in centers\n",
        "        )\n",
        "    best_pred_for_true_map = {i: (pk, d) for i, (pk, d) in enumerate(best_pred_list)}\n",
        "    min_true_over_preds_NW = np.array([d for (_, d) in best_pred_list], dtype=float)  # par true\n",
        "\n",
        "    # -------------------------------\n",
        "    # Matching glouton final (NW)\n",
        "    # -------------------------------\n",
        "    pairs = _greedy_match_from_candidates(candidates, n_true, threshold=THRESHOLD_PROX)\n",
        "    classes_retrouvees = len(pairs)\n",
        "    precision_classes = classes_retrouvees / max(1, n_pred)\n",
        "    recall_classes = classes_retrouvees / max(1, n_true)\n",
        "    f1_classes = (2 * precision_classes * recall_classes) / max(1e-12, precision_classes + recall_classes)\n",
        "    classes_en_trop = max(0, n_pred - classes_retrouvees)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Distances détaillées pour paires: NW et HW, direct vs reversed\n",
        "    # -------------------------------\n",
        "    def _pair_stats(ti: int, pk: int) -> dict:\n",
        "        tseq = centers[ti]\n",
        "        pseq = dict(pred_items)[pk]\n",
        "\n",
        "        # NW bi-orientation\n",
        "        best_NW, used_rev_NW, ddir_NW, drev_NW = _best_bidorient(pseq, tseq, mode=\"NW\")\n",
        "        # HW bi-orientation\n",
        "        best_HW, used_rev_HW, ddir_HW, drev_HW = _best_bidorient(pseq, tseq, mode=\"HW\")\n",
        "\n",
        "        return {\n",
        "            \"true_idx\": int(ti),\n",
        "            \"pred_key\": int(pk),\n",
        "            \"edit_distance_NW\": int(best_NW),\n",
        "            \"edit_distance_HW\": int(best_HW),\n",
        "            \"direct_NW\": int(ddir_NW),\n",
        "            \"reversed_NW\": int(drev_NW),\n",
        "            \"direct_HW\": int(ddir_HW),\n",
        "            \"reversed_HW\": int(drev_HW),\n",
        "            \"used_reversed_for_best_NW\": bool(used_rev_NW),\n",
        "            \"used_reversed_for_best_HW\": bool(used_rev_HW),\n",
        "            \"norm_edit_distance_NW\": float(best_NW / max(1, len(tseq))),\n",
        "            \"norm_edit_distance_HW\": float(best_HW / max(1, len(tseq))),\n",
        "        }\n",
        "\n",
        "    if pairs:\n",
        "        with tqdm_joblib(tqdm(desc=\"Pair stats (NW & HW, bi-orient)\", total=len(pairs))) as _:\n",
        "            pair_rows = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
        "                delayed(_pair_stats)(ti, pk) for (ti, pk, _) in pairs\n",
        "            )\n",
        "    else:\n",
        "        pair_rows = []\n",
        "\n",
        "    df_pairs = pd.DataFrame(pair_rows).sort_values([\"edit_distance_NW\", \"true_idx\"]) if pair_rows else pd.DataFrame(\n",
        "        columns=[\"true_idx\",\"pred_key\",\"edit_distance_NW\",\"edit_distance_HW\",\"direct_NW\",\"reversed_NW\",\"direct_HW\",\"reversed_HW\",\"used_reversed_for_best_NW\",\"used_reversed_for_best_HW\",\"norm_edit_distance_NW\",\"norm_edit_distance_HW\"]\n",
        "    )\n",
        "\n",
        "    # -------------------------------\n",
        "    # Agrégations et compteurs utiles\n",
        "    # -------------------------------\n",
        "    # Stats sur NW\n",
        "    dists_NW = df_pairs[\"edit_distance_NW\"].to_numpy() if not df_pairs.empty else np.array([], dtype=float)\n",
        "    if dists_NW.size:\n",
        "        avg_NW = float(dists_NW.mean()); med_NW = float(np.median(dists_NW))\n",
        "        p90_NW = float(np.percentile(dists_NW, 90)); max_NW = float(dists_NW.max())\n",
        "        exact_rate_NW = float((dists_NW == 0).mean())\n",
        "        c0_NW = int((dists_NW == 0).sum())\n",
        "        c1_NW = int((dists_NW == 1).sum())\n",
        "        c2_NW = int((dists_NW == 2).sum())\n",
        "        c3_NW = int((dists_NW == 3).sum())\n",
        "    else:\n",
        "        avg_NW = med_NW = p90_NW = max_NW = None\n",
        "        exact_rate_NW = 0.0\n",
        "        c0_NW = c1_NW = c2_NW = c3_NW = 0\n",
        "\n",
        "    # Stats sur HW\n",
        "    dists_HW = df_pairs[\"edit_distance_HW\"].to_numpy() if not df_pairs.empty else np.array([], dtype=float)\n",
        "    if dists_HW.size:\n",
        "        avg_HW = float(dists_HW.mean()); med_HW = float(np.median(dists_HW))\n",
        "        p90_HW = float(np.percentile(dists_HW, 90)); max_HW = float(dists_HW.max())\n",
        "        exact_rate_HW = float((dists_HW == 0).mean())\n",
        "        c0_HW = int((dists_HW == 0).sum())\n",
        "        c1_HW = int((dists_HW == 1).sum())\n",
        "        c2_HW = int((dists_HW == 2).sum())\n",
        "        c3_HW = int((dists_HW == 3).sum())\n",
        "    else:\n",
        "        avg_HW = med_HW = p90_HW = max_HW = None\n",
        "        exact_rate_HW = 0.0\n",
        "        c0_HW = c1_HW = c2_HW = c3_HW = 0\n",
        "\n",
        "    # Couverture TRUE -> PRED (NW bi-orientation)\n",
        "    seq_len = len(centers[0]) if n_true else (len(next(iter(pred_centers.values()))) if n_pred else 110)\n",
        "    df_mintrue = pd.DataFrame({\n",
        "        \"true_idx\": np.arange(n_true, dtype=int),\n",
        "        \"min_edit_distance_to_any_pred_NW\": min_true_over_preds_NW,\n",
        "        \"min_norm_edit_distance_to_any_pred_NW\": (min_true_over_preds_NW / max(1, seq_len))\n",
        "    })\n",
        "    finite_true = np.isfinite(df_mintrue[\"min_edit_distance_to_any_pred_NW\"].values)\n",
        "    if finite_true.any():\n",
        "        cover_mean = float(df_mintrue.loc[finite_true, \"min_edit_distance_to_any_pred_NW\"].mean())\n",
        "        cover_med  = float(df_mintrue.loc[finite_true, \"min_edit_distance_to_any_pred_NW\"].median())\n",
        "        cover_p90  = float(np.percentile(df_mintrue.loc[finite_true, \"min_edit_distance_to_any_pred_NW\"], 90))\n",
        "        covered_rate = (float((df_mintrue[\"min_edit_distance_to_any_pred_NW\"] <= THRESHOLD_PROX).mean())\n",
        "                        if THRESHOLD_PROX is not None else None)\n",
        "    else:\n",
        "        cover_mean = cover_med = cover_p90 = covered_rate = None\n",
        "\n",
        "    # Spécificité PRED -> TRUE (NW bi-orientation)\n",
        "    finite_pred = np.isfinite(min_pred_over_trues_NW)\n",
        "    if finite_pred.any():\n",
        "        spec_mean = float(min_pred_over_trues_NW[finite_pred].mean())\n",
        "        spec_med  = float(np.median(min_pred_over_trues_NW[finite_pred]))\n",
        "        spec_p90  = float(np.percentile(min_pred_over_trues_NW[finite_pred], 90))\n",
        "        specific_rate = (float((min_pred_over_trues_NW[finite_pred] <= THRESHOLD_PROX).mean())\n",
        "                         if THRESHOLD_PROX is not None else None)\n",
        "    else:\n",
        "        spec_mean = spec_med = spec_p90 = specific_rate = None\n",
        "\n",
        "    # Scores sous seuil (si fourni), sur les paires retenues, pour NW\n",
        "    if THRESHOLD_PROX is not None and dists_NW.size:\n",
        "        pairs_within_thr = int(np.sum(dists_NW <= THRESHOLD_PROX))\n",
        "        precision_at_thr = pairs_within_thr / max(1, n_pred)\n",
        "        recall_at_thr    = pairs_within_thr / max(1, n_true)\n",
        "        f1_at_thr        = (2 * precision_at_thr * recall_at_thr) / max(1e-12, precision_at_thr + recall_at_thr)\n",
        "    else:\n",
        "        pairs_within_thr = precision_at_thr = recall_at_thr = f1_at_thr = None\n",
        "\n",
        "    summary = {\n",
        "        # Comptage\n",
        "        \"n_true_centers\": int(n_true),\n",
        "        \"n_pred_centers\": int(n_pred),\n",
        "\n",
        "        # Matching glouton (NW bi-orient)\n",
        "        \"classes_retrouvees\": int(classes_retrouvees),\n",
        "        \"precision_classes\": float(precision_classes),\n",
        "        \"recall_classes\": float(recall_classes),\n",
        "        \"f1_classes\": float(f1_classes),\n",
        "        \"classes_en_trop\": int(classes_en_trop),\n",
        "\n",
        "        # Distances sur paires (NW)\n",
        "        \"avg_editdist_on_matched_NW\": avg_NW,\n",
        "        \"median_editdist_on_matched_NW\": med_NW,\n",
        "        \"p90_editdist_on_matched_NW\": p90_NW,\n",
        "        \"max_editdist_on_matched_NW\": max_NW,\n",
        "        \"exact_match_rate_on_matched_NW\": float(exact_rate_NW),\n",
        "        \"count_dist_0_NW\": int(c0_NW),\n",
        "        \"count_dist_1_NW\": int(c1_NW),\n",
        "        \"count_dist_2_NW\": int(c2_NW),\n",
        "        \"count_dist_3_NW\": int(c3_NW),\n",
        "\n",
        "        # Distances sur paires (HW)\n",
        "        \"avg_editdist_on_matched_HW\": avg_HW,\n",
        "        \"median_editdist_on_matched_HW\": med_HW,\n",
        "        \"p90_editdist_on_matched_HW\": p90_HW,\n",
        "        \"max_editdist_on_matched_HW\": max_HW,\n",
        "        \"exact_match_rate_on_matched_HW\": float(exact_rate_HW),\n",
        "        \"count_dist_0_HW\": int(c0_HW),\n",
        "        \"count_dist_1_HW\": int(c1_HW),\n",
        "        \"count_dist_2_HW\": int(c2_HW),\n",
        "        \"count_dist_3_HW\": int(c3_HW),\n",
        "\n",
        "        # Couverture vrais → prédits (NW bi-orient)\n",
        "        \"coverage_true_min_mean_NW\": cover_mean,\n",
        "        \"coverage_true_min_median_NW\": cover_med,\n",
        "        \"coverage_true_min_p90_NW\": cover_p90,\n",
        "        \"coverage_true_rate_below_threshold_NW\": covered_rate,\n",
        "\n",
        "        # Spécificité prédits → vrais (NW bi-orient)\n",
        "        \"specificity_pred_min_mean_NW\": spec_mean,\n",
        "        \"specificity_pred_min_median_NW\": spec_med,\n",
        "        \"specificity_pred_min_p90_NW\": spec_p90,\n",
        "        \"specificity_pred_rate_below_threshold_NW\": specific_rate,\n",
        "\n",
        "        # Scores sous seuil (si fourni) sur paires NW\n",
        "        \"pairs_within_threshold_NW\": pairs_within_thr,\n",
        "        \"precision_at_threshold_NW\": precision_at_thr,\n",
        "        \"recall_at_threshold_NW\": recall_at_thr,\n",
        "        \"f1_at_threshold_NW\": f1_at_thr,\n",
        "    }\n",
        "\n",
        "    return summary, df_pairs, df_mintrue\n",
        "\n",
        "\n",
        "# -----\n",
        "# Exemple:\n",
        "# -----\n",
        "THRESHOLD_PROX = 50\n",
        "summary, df_pairs, df_mintrue = compute_center_metrics_parallel(\n",
        "    centers, pred_centers, THRESHOLD_PROX=THRESHOLD_PROX, n_jobs=-1\n",
        ")\n",
        "print(summary)\n",
        "display(df_pairs.head(), df_mintrue.head())\n"
      ],
      "metadata": {
        "id": "CfkwNXwXKKW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}